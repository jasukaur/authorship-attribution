{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyarBgQsOmvz",
        "outputId": "d91e2dc5-4fb4-43f9-83d9-64016fca8996"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Conv1D, MaxPooling1D, LSTM, Dropout, Flatten, Bidirectional\n",
        "from keras.utils import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import drive\n",
        "# data_path = '/home/arsh/Jasleen/Spring 2023/NLP/Group Project/Authorship-Attribution-for-Neural-Text-Generation-master/data/'\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_path = 'drive/MyDrive/NLP/'\n",
        "data = pd.read_csv(data_path+'input.csv')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts = data['text'].tolist()\n",
        "classes = data['class'].tolist()\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "tokenized_texts = [word_tokenize(text.lower()) for text in texts]\n",
        "stop_words = set(stopwords.words('english'))\n",
        "punctuation = set(string.punctuation)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "preprocessed_texts = []\n",
        "for tokens in tokenized_texts:\n",
        "    filtered_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words and token not in punctuation]\n",
        "    preprocessed_texts.append(filtered_tokens)\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(preprocessed_texts)\n",
        "word_index = tokenizer.word_index\n",
        "sequences = tokenizer.texts_to_sequences(preprocessed_texts)\n",
        "max_sequence_length = 500\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_classes = label_encoder.fit_transform(classes)\n",
        "num_classes = len(set(classes))\n",
        "one_hot_classes = to_categorical(encoded_classes, num_classes=num_classes)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, one_hot_classes, test_size=0.2, random_state=42)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(word_index) + 1, 100, input_length=max_sequence_length))\n",
        "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=5))\n",
        "model.add(Bidirectional(LSTM(128)))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=128)\n",
        "\n",
        "model2 = Sequential()\n",
        "model2.add(Embedding(len(word_index) + 1, 100, input_length=max_sequence_length))\n",
        "model2.add(LSTM(128))\n",
        "model2.add(Dropout(0.2))\n",
        "model2.add(Dense(num_classes, activation='softmax'))\n",
        "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model2.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=128)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgDKLe1mOf0g",
        "outputId": "0e04c1ad-bd5b-4cc4-a970-57a11dc22dd2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "74/74 [==============================] - 27s 301ms/step - loss: 1.7215 - accuracy: 0.3769 - val_loss: 1.1390 - val_accuracy: 0.5708\n",
            "Epoch 2/5\n",
            "74/74 [==============================] - 17s 222ms/step - loss: 0.8077 - accuracy: 0.7023 - val_loss: 0.7997 - val_accuracy: 0.6931\n",
            "Epoch 3/5\n",
            "74/74 [==============================] - 18s 244ms/step - loss: 0.4507 - accuracy: 0.8350 - val_loss: 0.7356 - val_accuracy: 0.7323\n",
            "Epoch 4/5\n",
            "74/74 [==============================] - 22s 299ms/step - loss: 0.2355 - accuracy: 0.9203 - val_loss: 0.8772 - val_accuracy: 0.7310\n",
            "Epoch 5/5\n",
            "74/74 [==============================] - 13s 176ms/step - loss: 0.1032 - accuracy: 0.9706 - val_loss: 0.9572 - val_accuracy: 0.7374\n",
            "Epoch 1/5\n",
            "74/74 [==============================] - 19s 225ms/step - loss: 2.0323 - accuracy: 0.2243 - val_loss: 1.5750 - val_accuracy: 0.3815\n",
            "Epoch 2/5\n",
            "74/74 [==============================] - 17s 233ms/step - loss: 1.4413 - accuracy: 0.4275 - val_loss: 1.3832 - val_accuracy: 0.4774\n",
            "Epoch 3/5\n",
            "74/74 [==============================] - 16s 220ms/step - loss: 1.1499 - accuracy: 0.5949 - val_loss: 1.2357 - val_accuracy: 0.5618\n",
            "Epoch 4/5\n",
            "74/74 [==============================] - 15s 205ms/step - loss: 0.7398 - accuracy: 0.7599 - val_loss: 1.0760 - val_accuracy: 0.6373\n",
            "Epoch 5/5\n",
            "74/74 [==============================] - 14s 196ms/step - loss: 0.3793 - accuracy: 0.8833 - val_loss: 1.1074 - val_accuracy: 0.6513\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4846106bc0>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_lstm_predictions = model.predict(X_test)\n",
        "lstm_rnn_predictions = model2.predict(X_test)\n",
        "\n",
        "combined_predictions = []\n",
        "for i in range(len(cnn_lstm_predictions)):\n",
        "    combined_predictions.append(np.argmax(cnn_lstm_predictions[i] + lstm_rnn_predictions[i]))\n",
        "\n",
        "combined_accuracy = accuracy_score(np.argmax(y_test, axis=1), combined_predictions)\n",
        "print(\"Combined model accuracy:\", combined_accuracy)\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(np.argmax(y_test, axis=1), combined_predictions,target_names=label_encoder.classes_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCi-9bE5QmJH",
        "outputId": "6da4a656-55e4-4c43-9d1a-68607a53814f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "74/74 [==============================] - 1s 7ms/step\n",
            "74/74 [==============================] - 1s 9ms/step\n",
            "Combined model accuracy: 0.7502131287297528\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        ctrl       0.95      0.93      0.94       214\n",
            "        fair       0.53      0.70      0.60       209\n",
            "         gpt       1.00      1.00      1.00       217\n",
            "        gpt2       0.64      0.60      0.62       213\n",
            "        gpt3       0.48      0.55      0.51       217\n",
            "      grover       0.56      0.56      0.56       222\n",
            "       human       0.72      0.72      0.72       220\n",
            " instructgpt       0.82      0.51      0.63       202\n",
            "        pplm       0.79      0.75      0.77       206\n",
            "         xlm       0.98      0.97      0.97       215\n",
            "       xlnet       0.92      0.97      0.94       211\n",
            "\n",
            "    accuracy                           0.75      2346\n",
            "   macro avg       0.76      0.75      0.75      2346\n",
            "weighted avg       0.76      0.75      0.75      2346\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXO3jzc05k-h",
        "outputId": "c6693de6-f55a-4eca-cff2-ac43a49e639b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/charansaisadla/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/charansaisadla/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-05-09 12:55:35.644279: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "37/37 [==============================] - 28s 745ms/step - loss: 1.8806 - accuracy: 0.3483 - val_loss: 1.1191 - val_accuracy: 0.6053\n",
            "Epoch 2/5\n",
            "37/37 [==============================] - 29s 794ms/step - loss: 0.8174 - accuracy: 0.7130 - val_loss: 0.7992 - val_accuracy: 0.7221\n",
            "Epoch 3/5\n",
            "37/37 [==============================] - 29s 785ms/step - loss: 0.4261 - accuracy: 0.8592 - val_loss: 0.7758 - val_accuracy: 0.7361\n",
            "Epoch 4/5\n",
            "37/37 [==============================] - 29s 785ms/step - loss: 0.1786 - accuracy: 0.9478 - val_loss: 0.8523 - val_accuracy: 0.7327\n",
            "Epoch 5/5\n",
            "37/37 [==============================] - 29s 779ms/step - loss: 0.0675 - accuracy: 0.9856 - val_loss: 0.9687 - val_accuracy: 0.7421\n",
            "74/74 [==============================] - 3s 41ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.94      0.93       214\n",
            "           1       0.50      0.78      0.61       209\n",
            "           2       1.00      1.00      1.00       217\n",
            "           3       0.74      0.37      0.49       213\n",
            "           4       0.55      0.46      0.50       217\n",
            "           5       0.49      0.60      0.54       222\n",
            "           6       0.87      0.66      0.75       220\n",
            "           7       0.69      0.66      0.67       202\n",
            "           8       0.67      0.81      0.73       206\n",
            "           9       0.99      0.95      0.97       215\n",
            "          10       0.92      0.96      0.94       211\n",
            "\n",
            "    accuracy                           0.74      2346\n",
            "   macro avg       0.76      0.74      0.74      2346\n",
            "weighted avg       0.76      0.74      0.74      2346\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Conv1D, MaxPooling1D, LSTM, Dropout, Flatten, Bidirectional\n",
        "from keras.utils import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "\n",
        "# Step 1: Load the data from the CSV file\n",
        "\n",
        "data = pd.read_csv(\"/Users/charansaisadla/Downloads/input-2.csv\")\n",
        "\n",
        "texts = data['text'].tolist()\n",
        "classes = data['class'].tolist()\n",
        "\n",
        "# Step 2: Convert the text data into numerical representations\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "tokenized_texts = []\n",
        "for text in texts:\n",
        "    tokens = [lemmatizer.lemmatize(token.lower()) for token in word_tokenize(text) if\n",
        "              token.lower() not in stop_words and token.lower() not in string.punctuation]\n",
        "    tokenized_texts.append(tokens)\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(tokenized_texts)\n",
        "word_index = tokenizer.word_index\n",
        "sequences = tokenizer.texts_to_sequences(tokenized_texts)\n",
        "max_sequence_length = 500\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
        "\n",
        "# Step 3: Encode the class labels\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_classes = label_encoder.fit_transform(classes)\n",
        "num_classes = len(label_encoder.classes_)\n",
        "one_hot_classes = to_categorical(encoded_classes, num_classes=num_classes)\n",
        "\n",
        "# Step 4: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, one_hot_classes, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: Define and train the CRNN model\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(word_index) + 1, 100, input_length=max_sequence_length))\n",
        "model.add(Conv1D(256, 8, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=5))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=256)\n",
        "\n",
        "# Step 6: Evaluate the model on the test set\n",
        "y_pred_prob = model.predict(X_test)\n",
        "y_pred = y_pred_prob.argmax(axis=1)\n",
        "y_test_encoded = y_test.argmax(axis=1)\n",
        "report = classification_report(y_test_encoded, y_pred)\n",
        "print(report)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H66CTAXqOFlP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}