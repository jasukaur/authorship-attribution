{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/charansaisadla/miniconda3/envs/TrialEnv1/lib/python3.10/site-packages (4.28.1)\n",
      "Requirement already satisfied: filelock in /Users/charansaisadla/miniconda3/envs/TrialEnv1/lib/python3.10/site-packages (from transformers) (3.12.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /Users/charansaisadla/miniconda3/envs/TrialEnv1/lib/python3.10/site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/charansaisadla/miniconda3/envs/TrialEnv1/lib/python3.10/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/charansaisadla/miniconda3/envs/TrialEnv1/lib/python3.10/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/charansaisadla/miniconda3/envs/TrialEnv1/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/charansaisadla/miniconda3/envs/TrialEnv1/lib/python3.10/site-packages (from transformers) (2023.5.5)\n",
      "Requirement already satisfied: requests in /Users/charansaisadla/miniconda3/envs/TrialEnv1/lib/python3.10/site-packages (from transformers) (2.29.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/charansaisadla/miniconda3/envs/TrialEnv1/lib/python3.10/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/charansaisadla/miniconda3/envs/TrialEnv1/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /Users/charansaisadla/miniconda3/envs/TrialEnv1/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/charansaisadla/miniconda3/envs/TrialEnv1/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/charansaisadla/miniconda3/envs/TrialEnv1/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/charansaisadla/miniconda3/envs/TrialEnv1/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/charansaisadla/miniconda3/envs/TrialEnv1/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/charansaisadla/miniconda3/envs/TrialEnv1/lib/python3.10/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: torch in /Users/charansaisadla/miniconda3/envs/TrialEnv1/lib/python3.10/site-packages (2.0.0)\n",
      "Requirement already satisfied: filelock in /Users/charansaisadla/miniconda3/envs/TrialEnv1/lib/python3.10/site-packages (from torch) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/charansaisadla/miniconda3/envs/TrialEnv1/lib/python3.10/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: sympy in /Users/charansaisadla/miniconda3/envs/TrialEnv1/lib/python3.10/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/charansaisadla/miniconda3/envs/TrialEnv1/lib/python3.10/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/charansaisadla/miniconda3/envs/TrialEnv1/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/charansaisadla/miniconda3/envs/TrialEnv1/lib/python3.10/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/charansaisadla/miniconda3/envs/TrialEnv1/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_path = '../data/'\n",
    "df=pd.read_csv(data_path+\"input.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class'] = df['class'].apply(lambda x: 1 if x==7 else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are using the BERTforsequenceClassification which can be used as classifier,here we are using from_pretained is which there are several models are available for bert ex:bert-large-cased,bert-base-cased with several change in parameters but the basic and efficient one we are using bert_base-uncasedd here.\n",
    "\n",
    "The number of labels mentioned here is 2 which denotes that it has to classfiy into 2 classes.\n",
    "\n",
    "The tokenizer we are using BERTTokenizer which can be used as tokenize the input and feed it into the BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charansaisadla/miniconda3/envs/TrialEnv1/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-cased', num_labels=2)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so here we are defining a function in which is going to tokenize and encode together whihc passing the inputs text,tokenizer,maxlenght.\n",
    "\n",
    "encode_plus() method of the tokenizer object. The encode_plus() method takes in the text input, adds special tokens to mark the beginning and end of the sequence, truncates or pads the sequence to the specified max_length, and returns a dictionary containing the encoded sequence, the attention mask, and the token type IDs.\n",
    "\n",
    "so usually needed the encoded inputs and attentionmask to know which we can feed and which we can skip.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def tokenize_and_encode(text, tokenizer, max_length):\n",
    "    input_ids = tokenizer.encode_plus(text, add_special_tokens=True, max_length=max_length, padding='max_length', truncation=True, return_token_type_ids=False, return_attention_mask=True, return_tensors='pt')\n",
    "    return input_ids['input_ids'], input_ids['attention_mask']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_ids = torch.cat(input_ids, dim=0): Concatenates the list of encoded input sequences into a single PyTorch tensor along the 0th dimension.\n",
    "attention_masks = torch.cat(attention_masks, dim=0): Concatenates the list of attention masks into a single PyTorch tensor along the 0th dimension.\n",
    "labels = torch.tensor(labels): Converts the list of class labels to a PyTorch tensor.\n",
    "\n",
    "TensorDataset object using the input sequences, attention masks, and labels that we have converted to PyTorch tensors. This TensorDataset object then serves as the input to the DataLoader.\n",
    "\n",
    "The DataLoader iterates over the TensorDataset object in batches, shuffling the data if shuffle=True. During each iteration, the DataLoader returns a batch of input sequences, attention masks, and labels, which can then be used to train or evaluate the model.\n",
    "\n",
    "By using DataLoader to iterate over the dataset, we can effectively process large amounts of data in batches, while minimizing the amount of memory required to store the entire dataset in memory at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(df, tokenizer, max_length, batch_size, shuffle=True):\n",
    "    texts = df['text'].values\n",
    "    labels = df['class'].values\n",
    "    \n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    for text in texts:\n",
    "        input_id, attention_mask = tokenize_and_encode(text, tokenizer, max_length)\n",
    "        input_ids.append(input_id)\n",
    "        attention_masks.append(attention_mask)\n",
    "        \n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(labels)\n",
    "    \n",
    "    dataset = torch.utils.data.TensorDataset(input_ids, attention_masks, labels)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    \n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, attention_masks, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_masks, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def evaluate_model(model, val_dataloader, device):\n",
    "    model.eval()\n",
    "    val_loss, val_accuracy = 0, 0\n",
    "    n_val_steps = 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, attention_masks, labels = batch\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_masks, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds = logits.argmax(dim=1)\n",
    "            all_preds.extend(preds.tolist())\n",
    "            all_labels.extend(labels.tolist())\n",
    "            val_accuracy += (preds == labels).float().mean().item()\n",
    "            n_val_steps += 1\n",
    "\n",
    "    val_loss /= n_val_steps\n",
    "    val_accuracy /= n_val_steps\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    return val_loss, val_accuracy, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Val Loss: 0.066, Val Accuracy: 0.982, Val Precision: 0.982, Val Recall: 0.982, Val F1: 0.981\n",
      "Epoch 2 Val Loss: 0.053, Val Accuracy: 0.984, Val Precision: 0.984, Val Recall: 0.984, Val F1: 0.984\n",
      "Epoch 3 Val Loss: 0.025, Val Accuracy: 0.995, Val Precision: 0.995, Val Recall: 0.995, Val F1: 0.995\n",
      "Epoch 4 Val Loss: 0.046, Val Accuracy: 0.992, Val Precision: 0.992, Val Recall: 0.992, Val F1: 0.992\n",
      "Epoch 5 Val Loss: 0.056, Val Accuracy: 0.991, Val Precision: 0.992, Val Recall: 0.991, Val F1: 0.991\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 5\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-cased\").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "train_dataloader = get_dataloader(train_df, tokenizer, MAX_LENGTH, BATCH_SIZE)\n",
    "val_dataloader = get_dataloader(test_df, tokenizer, MAX_LENGTH, BATCH_SIZE)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_model(model, train_dataloader, optimizer, device)\n",
    "    val_loss, val_accuracy, val_precision, val_recall, val_f1 = evaluate_model(model, val_dataloader, device)\n",
    "    print(f\"Epoch {epoch + 1} Val Loss: {val_loss:.3f}, Val Accuracy: {val_accuracy:.3f}, Val Precision: {val_precision:.3f}, Val Recall: {val_recall:.3f}, Val F1: {val_f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "print(torch.backends.mps.is_available())\n",
    "# this ensures that the current current PyTorch installation was built with MPS activated.\n",
    "print(torch.backends.mps.is_built())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: Human\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "input_text = \"Cristiano Ronaldo is a Portuguese professional footballer widely regarded as one of the greatest players of all time. He was born on February 5, 1985, in Madeira, Portugal. Ronaldo began his football career at a young age, playing for Andorinha and later moving on to play for Nacional.At the age of just 16, Ronaldo signed with Sporting CP, one of the top football clubs in Portugal. He quickly made a name for himself, scoring several impressive goals and helping his team win important matches. In 2003, at the age of 18, Ronaldo signed with Manchester United, one of the most prestigious football clubs in the world.\"\n",
    "encoded_input = tokenizer(input_text, padding='max_length', truncation=True, max_length=MAX_LENGTH, return_tensors='pt')\n",
    "input_ids = encoded_input['input_ids'].to(device)\n",
    "attention_mask = encoded_input['attention_mask'].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "\n",
    "if predicted_class==1:\n",
    "    print(\"Predicted class: Human\")\n",
    "else:\n",
    "    print(\"machine\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.save(model.state_dict(), 'BERT p2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
