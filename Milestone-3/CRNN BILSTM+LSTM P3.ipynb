{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Conv1D, MaxPooling1D, LSTM, Dropout, Flatten, Bidirectional\n",
        "from keras.utils import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "data = x\n",
        "\n",
        "texts = data['text'].tolist()\n",
        "classes = data['class'].tolist()\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "tokenized_texts = [word_tokenize(text.lower()) for text in texts]\n",
        "stop_words = set(stopwords.words('english'))\n",
        "punctuation = set(string.punctuation)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "preprocessed_texts = []\n",
        "for tokens in tokenized_texts:\n",
        "    filtered_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words and token not in punctuation]\n",
        "    preprocessed_texts.append(filtered_tokens)\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(preprocessed_texts)\n",
        "word_index = tokenizer.word_index\n",
        "sequences = tokenizer.texts_to_sequences(preprocessed_texts)\n",
        "max_sequence_length = 500\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_classes = label_encoder.fit_transform(classes)\n",
        "num_classes = len(set(classes))\n",
        "one_hot_classes = to_categorical(encoded_classes, num_classes=num_classes)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, one_hot_classes, test_size=0.2, random_state=42)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(word_index) + 1, 100, input_length=max_sequence_length))\n",
        "model.add(Conv1D(filters=256, kernel_size=5, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=7))\n",
        "model.add(Bidirectional(LSTM(200)))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=128)\n",
        "\n",
        "model2 = Sequential()\n",
        "model2.add(Embedding(len(word_index) + 1, 100, input_length=max_sequence_length))\n",
        "model2.add(LSTM(200))\n",
        "model2.add(Dropout(0.2))\n",
        "model2.add(Dense(num_classes, activation='softmax'))\n",
        "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model2.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=128)\n",
        "\n",
        "cnn_lstm_predictions = model.predict(X_test)\n",
        "lstm_rnn_predictions = model2.predict(X_test)\n",
        "\n",
        "combined_predictions = []\n",
        "for i in range(len(cnn_lstm_predictions)):\n",
        "    combined_predictions.append(np.argmax(cnn_lstm_predictions[i] + lstm_rnn_predictions[i]))\n",
        "\n",
        "combined_accuracy = accuracy_score(np.argmax(y_test, axis=1), combined_predictions)\n",
        "print(\"Combined model accuracy:\", combined_accuracy)\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(np.argmax(y_test, axis=1), combined_predictions,target_names=label_encoder.classes_))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tor7rI57YwJN",
        "outputId": "6ae9cf6c-8bc5-4fb1-811e-66d9ae4d8faa"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "74/74 [==============================] - 25s 273ms/step - loss: 1.4547 - accuracy: 0.4544 - val_loss: 0.8246 - val_accuracy: 0.6935\n",
            "Epoch 2/5\n",
            "74/74 [==============================] - 15s 207ms/step - loss: 0.5316 - accuracy: 0.8072 - val_loss: 0.5268 - val_accuracy: 0.8082\n",
            "Epoch 3/5\n",
            "74/74 [==============================] - 15s 207ms/step - loss: 0.2028 - accuracy: 0.9336 - val_loss: 0.5309 - val_accuracy: 0.8163\n",
            "Epoch 4/5\n",
            "74/74 [==============================] - 15s 197ms/step - loss: 0.0689 - accuracy: 0.9799 - val_loss: 0.5941 - val_accuracy: 0.8257\n",
            "Epoch 5/5\n",
            "74/74 [==============================] - 15s 201ms/step - loss: 0.0299 - accuracy: 0.9922 - val_loss: 0.6516 - val_accuracy: 0.8231\n",
            "Epoch 1/5\n",
            "74/74 [==============================] - 22s 254ms/step - loss: 2.0726 - accuracy: 0.2760 - val_loss: 1.5213 - val_accuracy: 0.4659\n",
            "Epoch 2/5\n",
            "74/74 [==============================] - 16s 218ms/step - loss: 1.1862 - accuracy: 0.5693 - val_loss: 1.0971 - val_accuracy: 0.6006\n",
            "Epoch 3/5\n",
            "74/74 [==============================] - 15s 206ms/step - loss: 0.8710 - accuracy: 0.6921 - val_loss: 0.8221 - val_accuracy: 0.7038\n",
            "Epoch 4/5\n",
            "74/74 [==============================] - 14s 193ms/step - loss: 0.4118 - accuracy: 0.8485 - val_loss: 0.8295 - val_accuracy: 0.7306\n",
            "Epoch 5/5\n",
            "74/74 [==============================] - 15s 200ms/step - loss: 0.2423 - accuracy: 0.9212 - val_loss: 0.8105 - val_accuracy: 0.7421\n",
            "74/74 [==============================] - 1s 7ms/step\n",
            "74/74 [==============================] - 1s 9ms/step\n",
            "Combined model accuracy: 0.8256606990622336\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        ctrl       0.95      0.98      0.96       214\n",
            "        fair       1.00      1.00      1.00       217\n",
            "         gpt       0.72      0.61      0.66       213\n",
            "        gpt2       0.85      0.90      0.88       431\n",
            "        gpt3       0.99      0.95      0.97       215\n",
            "      grover       0.94      0.95      0.95       211\n",
            "       human       0.91      0.72      0.81       206\n",
            " instructgpt       0.79      0.74      0.77       220\n",
            "         xlm       0.51      0.65      0.57       217\n",
            "        xlne       0.66      0.67      0.66       202\n",
            "\n",
            "    accuracy                           0.83      2346\n",
            "   macro avg       0.83      0.82      0.82      2346\n",
            "weighted avg       0.83      0.83      0.83      2346\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Conv1D, MaxPooling1D, LSTM, Dropout, Flatten, Bidirectional\n",
        "from keras.utils import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "\n",
        "# Step 1: Load the data from the CSV file\n",
        "\n",
        "data = pd.read_csv(\"input.csv\")\n",
        "\n",
        "texts = data['text'].tolist()\n",
        "classes = data['class'].tolist()\n",
        "\n",
        "# Step 2: Convert the text data into numerical representations\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "tokenized_texts = []\n",
        "for text in texts:\n",
        "    tokens = [lemmatizer.lemmatize(token.lower()) for token in word_tokenize(text) if\n",
        "              token.lower() not in stop_words and token.lower() not in string.punctuation]\n",
        "    tokenized_texts.append(tokens)\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(tokenized_texts)\n",
        "word_index = tokenizer.word_index\n",
        "sequences = tokenizer.texts_to_sequences(tokenized_texts)\n",
        "max_sequence_length = 500\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
        "\n",
        "# Step 3: Encode the class labels\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_classes = label_encoder.fit_transform(classes)\n",
        "num_classes = len(label_encoder.classes_)\n",
        "one_hot_classes = to_categorical(encoded_classes, num_classes=num_classes)\n",
        "\n",
        "# Step 4: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, one_hot_classes, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: Define and train the CRNN model\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(word_index) + 1, 100, input_length=max_sequence_length))\n",
        "model.add(Conv1D(256, 8, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=5))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=256)\n",
        "\n",
        "# Step 6: Evaluate the model on the test set\n",
        "y_pred_prob = model.predict(X_test)\n",
        "y_pred = y_pred_prob.argmax(axis=1)\n",
        "y_test_encoded = y_test.argmax(axis=1)\n",
        "\n",
        "# Step 7: Decode the class labels and generate the classification report\n",
        "decoded_classes = label_encoder.inverse_transform(range(num_classes))\n",
        "report = classification_report(y_test_encoded, y_pred, target_names=decoded_classes)\n",
        "print(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhYHkKkX6mcS",
        "outputId": "4477fd9c-c3f5-409b-8550-7f9eda116781"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "37/37 [==============================] - 15s 317ms/step - loss: 1.8893 - accuracy: 0.3239 - val_loss: 1.0933 - val_accuracy: 0.5904\n",
            "Epoch 2/5\n",
            "37/37 [==============================] - 10s 269ms/step - loss: 0.6412 - accuracy: 0.7732 - val_loss: 0.5880 - val_accuracy: 0.7877\n",
            "Epoch 3/5\n",
            "37/37 [==============================] - 10s 263ms/step - loss: 0.2253 - accuracy: 0.9284 - val_loss: 0.5576 - val_accuracy: 0.8248\n",
            "Epoch 4/5\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 0.0696 - accuracy: 0.9827 - val_loss: 0.6190 - val_accuracy: 0.8248\n",
            "Epoch 5/5\n",
            "37/37 [==============================] - 9s 256ms/step - loss: 0.0382 - accuracy: 0.9912 - val_loss: 0.6954 - val_accuracy: 0.8163\n",
            "74/74 [==============================] - 1s 5ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        ctrl       0.93      0.94      0.94       214\n",
            "        fair       1.00      1.00      1.00       217\n",
            "         gpt       0.78      0.57      0.66       213\n",
            "        gpt2       0.74      0.95      0.83       431\n",
            "        gpt3       0.98      0.98      0.98       215\n",
            "      grover       0.96      0.94      0.95       211\n",
            "       human       0.81      0.78      0.80       206\n",
            " instructgpt       0.92      0.65      0.76       220\n",
            "         xlm       0.53      0.58      0.56       217\n",
            "        xlne       0.68      0.65      0.66       202\n",
            "\n",
            "    accuracy                           0.82      2346\n",
            "   macro avg       0.83      0.80      0.81      2346\n",
            "weighted avg       0.82      0.82      0.81      2346\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "STVcdrOzGEDv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}