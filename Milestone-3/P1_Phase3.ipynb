{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "BPoa3D7zcF1K"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense"
      ],
      "metadata": {
        "id": "piYLfO46jL2R"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "PbbzS_H2cF1T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb0c7859-47b8-4033-d7cb-0128636a8ad1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# # data_path = '/home/arsh/Jasleen/Spring 2023/NLP/Group Project/Authorship-Attribution-for-Neural-Text-Generation-master/data/'\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = 'drive/MyDrive/Colab Notebooks/'"
      ],
      "metadata": {
        "id": "qk-SUeHMdFRp"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(data_path+'input.csv')\n",
        "# data.tail()"
      ],
      "metadata": {
        "id": "rTu7oIwWjdC2"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #Creating paired dataset\n",
        "\n",
        "\n",
        "# import pandas as pd\n",
        "# import itertools\n",
        "\n",
        "# # load the input file\n",
        "# # df = pd.read_csv('input.csv')\n",
        "# df = data\n",
        "\n",
        "# # get 100 samples of each NLG method\n",
        "# sample_size = 50\n",
        "# df_sample = pd.DataFrame()\n",
        "# for class_ in df['class'].unique():\n",
        "#     df_class = df[df['class'] == class_].sample(n=sample_size, random_state=1)\n",
        "#     df_sample = pd.concat([df_sample, df_class], ignore_index=True)\n",
        "\n",
        "# # create another dataframe with 3 columns: prompt1, prompt2, and label\n",
        "# df_new = pd.DataFrame(columns=['prompt1', 'prompt2', 'label'])\n",
        "\n",
        "# # pair each text with all other texts in df_sample\n",
        "# for _, row1 in df_sample.iterrows():\n",
        "#     for _, row2 in df_sample.iterrows():\n",
        "#         if row1['text'] != row2['text']:\n",
        "#             if row1['class'] == row2['class']:\n",
        "#                 label = 0\n",
        "#             else:\n",
        "#                 label = 1\n",
        "#             df_new = df_new.append({'prompt1': row1['text'], 'prompt2': row2['text'], 'label': label}, ignore_index=True)\n",
        "\n",
        "# # save the new file as csv\n",
        "# df_new.to_csv('output.csv', index=False)\n"
      ],
      "metadata": {
        "id": "MUhp8sFKUnFq"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating new pairs\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the data from the CSV file\n",
        "# df = pd.read_csv('input.csv')\n",
        "df=data\n",
        "\n",
        "# Select only 100 samples from each class\n",
        "grouped = df.groupby('class').apply(lambda x: x.sample(n=min(len(x), 1000))).reset_index(drop=True)\n",
        "\n",
        "# Group the selected data by the \"class\" column\n",
        "grouped = grouped.groupby('class')\n",
        "\n",
        "# Randomly select 10 rows from each group\n",
        "samples = []\n",
        "for _, group in grouped:\n",
        "    samples.extend(list(zip(group.sample(n=1000, replace=True).index, \n",
        "                             group.sample(n=1000, replace=True).index)))\n",
        "\n",
        "\n",
        "# Generate pairs from the selected rows and random rows from other classes\n",
        "pairs = []\n",
        "labels = []\n",
        "for i, j in samples:\n",
        "    # Positive pair\n",
        "    if df['class'][i] == df['class'][j]:\n",
        "        pairs.append([df['text'][i], df['text'][j]])\n",
        "        labels.append(0)\n",
        "\n",
        "    # Negative pair\n",
        "    else:\n",
        "        while True:\n",
        "            k = np.random.choice(df.index)\n",
        "            if df['class'][k] != df['class'][i]:\n",
        "                pairs.append([df['text'][i], df['text'][k]])\n",
        "                labels.append(1)\n",
        "                break\n",
        "\n",
        "# Create a new dataframe with the pairs and labels\n",
        "new_df = pd.DataFrame({'prompt1': [p[0] for p in pairs], \n",
        "                       'prompt2': [p[1] for p in pairs], \n",
        "                       'class': labels})\n",
        "\n",
        "# Save the new dataframe to a CSV file\n",
        "new_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "9Oznd2V6XdVF",
        "outputId": "cc9f8593-ba92-4c9d-dd8f-c80009de1d2c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 prompt1  \\\n",
              "0      news kenyas best food is a kaleidoscope of fla...   \n",
              "1      news nbas coronavirus hiatus could mean chaos ...   \n",
              "2      news step into the world of americas most noto...   \n",
              "3      news tokyos shibuya crossing welcome to the wo...   \n",
              "4      news one man lost his life savings in a sim ha...   \n",
              "...                                                  ...   \n",
              "10995  does being tiktok famous actually make you mon...   \n",
              "10996  girls behind bars tell their stories girls beh...   \n",
              "10997  government says four cancer charities are sham...   \n",
              "10998  how nasas apollo program kicked off silicon va...   \n",
              "10999  welcome to walmart the doctor will see you now...   \n",
              "\n",
              "                                                 prompt2  class  \n",
              "0      news incredible pictures capture rare elephant...      0  \n",
              "1      news best diets ranking puts keto last dash fi...      0  \n",
              "2      news politicians live it up and have the lobby...      0  \n",
              "3      news goodbye to he and she and hello to ze n t...      0  \n",
              "4      news chinese restaurants are losing business o...      0  \n",
              "...                                                  ...    ...  \n",
              "10995  the biggest surprise in trump trial the bigges...      0  \n",
              "10996  news george takei on this remembrance day i he...      1  \n",
              "10997  in 2017 trump revived feminism saved satire an...      1  \n",
              "10998  coronavirus is fast becoming an economic pande...      1  \n",
              "10999  el helicoide the futuristic wonder that now su...      1  \n",
              "\n",
              "[11000 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dae6c3bd-9eb0-42d1-8a67-79432c6a5873\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>prompt1</th>\n",
              "      <th>prompt2</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>news kenyas best food is a kaleidoscope of fla...</td>\n",
              "      <td>news incredible pictures capture rare elephant...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>news nbas coronavirus hiatus could mean chaos ...</td>\n",
              "      <td>news best diets ranking puts keto last dash fi...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>news step into the world of americas most noto...</td>\n",
              "      <td>news politicians live it up and have the lobby...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>news tokyos shibuya crossing welcome to the wo...</td>\n",
              "      <td>news goodbye to he and she and hello to ze n t...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>news one man lost his life savings in a sim ha...</td>\n",
              "      <td>news chinese restaurants are losing business o...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10995</th>\n",
              "      <td>does being tiktok famous actually make you mon...</td>\n",
              "      <td>the biggest surprise in trump trial the bigges...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10996</th>\n",
              "      <td>girls behind bars tell their stories girls beh...</td>\n",
              "      <td>news george takei on this remembrance day i he...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10997</th>\n",
              "      <td>government says four cancer charities are sham...</td>\n",
              "      <td>in 2017 trump revived feminism saved satire an...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10998</th>\n",
              "      <td>how nasas apollo program kicked off silicon va...</td>\n",
              "      <td>coronavirus is fast becoming an economic pande...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10999</th>\n",
              "      <td>welcome to walmart the doctor will see you now...</td>\n",
              "      <td>el helicoide the futuristic wonder that now su...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11000 rows Ã— 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dae6c3bd-9eb0-42d1-8a67-79432c6a5873')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-dae6c3bd-9eb0-42d1-8a67-79432c6a5873 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-dae6c3bd-9eb0-42d1-8a67-79432c6a5873');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM"
      ],
      "metadata": {
        "id": "B0-tkoAaDh2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple Neural Model\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
        "\n",
        "# Read in the dataset\n",
        "df = new_df\n",
        "\n",
        "# Split the dataset into samples with label 0 and 1\n",
        "label_0_df = df[df['class'] == 0].sample(n=1000, random_state=42)\n",
        "label_1_df = df[df['class'] == 1].sample(n=1000, random_state=42)\n",
        "\n",
        "# Combine the two samples\n",
        "sample_df = pd.concat([label_0_df, label_1_df])\n",
        "\n",
        "# Convert the text prompts to numerical features using Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sample_df[['prompt1', 'prompt2']].apply(lambda x: ' '.join(x), axis=1))\n",
        "X = tokenizer.texts_to_sequences(sample_df[['prompt1', 'prompt2']].apply(lambda x: ' '.join(x), axis=1))\n",
        "\n",
        "# Pad the sequences to a fixed length\n",
        "max_length = 300\n",
        "X = tf.keras.preprocessing.sequence.pad_sequences(X, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "# Add a third dimension to X\n",
        "X = np.expand_dims(X, axis=2)\n",
        "\n",
        "# Convert the labels to be in the range [0, 1]\n",
        "y = sample_df['class'].values\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model architecture\n",
        "model = Sequential([\n",
        "    LSTM(64, input_shape=(max_length, 1)),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model on test data\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f'Test loss: {score[0]}')\n",
        "print(f'Test accuracy: {score[1]}')\n"
      ],
      "metadata": {
        "id": "6USkPxctU2a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "283595fb-6051-4e9b-d034-c518083e5624"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "50/50 [==============================] - 9s 24ms/step - loss: 0.7657 - accuracy: 0.4888 - val_loss: 0.6936 - val_accuracy: 0.5050\n",
            "Epoch 2/10\n",
            "50/50 [==============================] - 1s 12ms/step - loss: 0.7395 - accuracy: 0.5025 - val_loss: 0.6922 - val_accuracy: 0.5175\n",
            "Epoch 3/10\n",
            "50/50 [==============================] - 1s 13ms/step - loss: 0.7278 - accuracy: 0.5319 - val_loss: 0.6931 - val_accuracy: 0.4975\n",
            "Epoch 4/10\n",
            "50/50 [==============================] - 1s 12ms/step - loss: 0.7187 - accuracy: 0.5138 - val_loss: 0.6913 - val_accuracy: 0.5225\n",
            "Epoch 5/10\n",
            "50/50 [==============================] - 1s 12ms/step - loss: 0.7223 - accuracy: 0.5081 - val_loss: 0.6914 - val_accuracy: 0.5375\n",
            "Epoch 6/10\n",
            "50/50 [==============================] - 1s 12ms/step - loss: 0.7195 - accuracy: 0.4819 - val_loss: 0.6881 - val_accuracy: 0.5425\n",
            "Epoch 7/10\n",
            "50/50 [==============================] - 1s 11ms/step - loss: 0.7040 - accuracy: 0.5306 - val_loss: 0.6928 - val_accuracy: 0.5275\n",
            "Epoch 8/10\n",
            "50/50 [==============================] - 1s 11ms/step - loss: 0.7027 - accuracy: 0.5250 - val_loss: 0.6920 - val_accuracy: 0.5025\n",
            "Epoch 9/10\n",
            "50/50 [==============================] - 1s 14ms/step - loss: 0.7034 - accuracy: 0.5125 - val_loss: 0.6896 - val_accuracy: 0.5450\n",
            "Epoch 10/10\n",
            "50/50 [==============================] - 1s 17ms/step - loss: 0.6916 - accuracy: 0.5431 - val_loss: 0.6876 - val_accuracy: 0.5450\n",
            "Test loss: 0.6876187920570374\n",
            "Test accuracy: 0.5450000166893005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BidirectionalLSTM"
      ],
      "metadata": {
        "id": "v_9-fWqODkGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM, Bidirectional, Embedding, SpatialDropout1D\n",
        "\n",
        "# Read in the dataset\n",
        "# df = pd.read_csv('p1_dataset.csv')\n",
        "df = new_df\n",
        "\n",
        "# # Split the dataset into samples with label 0 and 1\n",
        "# label_0_df = df[df['class'] == 0].sample(n=10000, random_state=42)\n",
        "# label_1_df = df[df['class'] == 1].sample(n=10000, random_state=42)\n",
        "\n",
        "# Sample from each label, reducing the requested size if necessary\n",
        "sample_size = 10000\n",
        "if len(label_0_df) < sample_size:\n",
        "    sample_size = len(label_0_df)\n",
        "if len(label_1_df) < sample_size:\n",
        "    sample_size = len(label_1_df)\n",
        "\n",
        "label_0_df = label_0_df.sample(n=sample_size, random_state=42, replace=True)\n",
        "label_1_df = label_1_df.sample(n=sample_size, random_state=42, replace=True)\n",
        "\n",
        "# Combine the two samples\n",
        "sample_df = pd.concat([label_0_df, label_1_df])\n",
        "\n",
        "# Convert the text prompts to numerical features using Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sample_df[['prompt1', 'prompt2']].apply(lambda x: ' '.join(x), axis=1))\n",
        "X = tokenizer.texts_to_sequences(sample_df[['prompt1', 'prompt2']].apply(lambda x: ' '.join(x), axis=1))\n",
        "\n",
        "# Pad the sequences to a fixed length\n",
        "max_length = 300\n",
        "X = tf.keras.preprocessing.sequence.pad_sequences(X, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "# Add a third dimension to X\n",
        "X = np.expand_dims(X, axis=2)\n",
        "\n",
        "# Convert the labels to be in the range [0, 1]\n",
        "y = sample_df['class'].values\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model architecture\n",
        "model = Sequential([\n",
        "    Embedding(len(tokenizer.word_index) + 1, 128, input_length=max_length),\n",
        "    SpatialDropout1D(0.2),\n",
        "    Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2)),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model on test data\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f'Test loss: {score[0]}')\n",
        "print(f'Test accuracy: {score[1]}')\n"
      ],
      "metadata": {
        "id": "pizMks9uqniN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32b83c7c-e8ad-4719-b02a-48dc65511ec9"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "50/50 [==============================] - 72s 1s/step - loss: 0.6861 - accuracy: 0.5794 - val_loss: 0.6655 - val_accuracy: 0.5975\n",
            "Epoch 2/10\n",
            "50/50 [==============================] - 67s 1s/step - loss: 0.4987 - accuracy: 0.7850 - val_loss: 0.5688 - val_accuracy: 0.6900\n",
            "Epoch 3/10\n",
            "50/50 [==============================] - 67s 1s/step - loss: 0.1741 - accuracy: 0.9531 - val_loss: 0.6945 - val_accuracy: 0.7125\n",
            "Epoch 4/10\n",
            "50/50 [==============================] - 66s 1s/step - loss: 0.0838 - accuracy: 0.9775 - val_loss: 0.7819 - val_accuracy: 0.7225\n",
            "Epoch 5/10\n",
            "50/50 [==============================] - 65s 1s/step - loss: 0.0810 - accuracy: 0.9775 - val_loss: 0.8981 - val_accuracy: 0.6975\n",
            "Epoch 6/10\n",
            "50/50 [==============================] - 63s 1s/step - loss: 0.0580 - accuracy: 0.9812 - val_loss: 0.9787 - val_accuracy: 0.7025\n",
            "Epoch 7/10\n",
            "50/50 [==============================] - 63s 1s/step - loss: 0.0492 - accuracy: 0.9819 - val_loss: 0.8835 - val_accuracy: 0.7450\n",
            "Epoch 8/10\n",
            "50/50 [==============================] - 63s 1s/step - loss: 0.0419 - accuracy: 0.9787 - val_loss: 0.9760 - val_accuracy: 0.7150\n",
            "Epoch 9/10\n",
            "50/50 [==============================] - 63s 1s/step - loss: 0.0386 - accuracy: 0.9844 - val_loss: 0.8986 - val_accuracy: 0.7200\n",
            "Epoch 10/10\n",
            "50/50 [==============================] - 62s 1s/step - loss: 0.0388 - accuracy: 0.9837 - val_loss: 1.1058 - val_accuracy: 0.7200\n",
            "Test loss: 1.1058363914489746\n",
            "Test accuracy: 0.7200000286102295\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BI-LSTm with L2 Regularizer"
      ],
      "metadata": {
        "id": "SfcDHn97Do1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM, Bidirectional, Embedding, SpatialDropout1D\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "# Read in the dataset\n",
        "# df = pd.read_csv('p1_dataset.csv')\n",
        "df = new_df\n",
        "\n",
        "# Sample from each label, reducing the requested size if necessary\n",
        "sample_size = 10000\n",
        "if len(label_0_df) < sample_size:\n",
        "    sample_size = len(label_0_df)\n",
        "if len(label_1_df) < sample_size:\n",
        "    sample_size = len(label_1_df)\n",
        "\n",
        "label_0_df = label_0_df.sample(n=sample_size, random_state=42, replace=True)\n",
        "label_1_df = label_1_df.sample(n=sample_size, random_state=42, replace=True)\n",
        "\n",
        "# Combine the two samples\n",
        "sample_df = pd.concat([label_0_df, label_1_df])\n",
        "\n",
        "# Convert the text prompts to numerical features using Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sample_df[['prompt1', 'prompt2']].apply(lambda x: ' '.join(x), axis=1))\n",
        "X = tokenizer.texts_to_sequences(sample_df[['prompt1', 'prompt2']].apply(lambda x: ' '.join(x), axis=1))\n",
        "\n",
        "# Pad the sequences to a fixed length\n",
        "max_length = 300\n",
        "X = tf.keras.preprocessing.sequence.pad_sequences(X, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "# Add a third dimension to X\n",
        "X = np.expand_dims(X, axis=2)\n",
        "\n",
        "# Convert the labels to be in the range [0, 1]\n",
        "y = sample_df['class'].values\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model architecture\n",
        "model = Sequential([\n",
        "    Embedding(len(tokenizer.word_index) + 1, 128, input_length=max_length),\n",
        "    SpatialDropout1D(0.2),\n",
        "    Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2)),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid', kernel_regularizer=l2(0.01))\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model on test data\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f'Test loss: {score[0]}')\n",
        "print(f'Test accuracy: {score[1]}')\n"
      ],
      "metadata": {
        "id": "giHiLqwfCDqj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51f55535-a263-4bd0-ef29-f3ba5e88688f"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "50/50 [==============================] - 71s 1s/step - loss: 0.7023 - accuracy: 0.5544 - val_loss: 0.6824 - val_accuracy: 0.6450\n",
            "Epoch 2/10\n",
            "50/50 [==============================] - 67s 1s/step - loss: 0.5693 - accuracy: 0.7731 - val_loss: 0.5406 - val_accuracy: 0.7325\n",
            "Epoch 3/10\n",
            "50/50 [==============================] - 66s 1s/step - loss: 0.2292 - accuracy: 0.9300 - val_loss: 0.5108 - val_accuracy: 0.8025\n",
            "Epoch 4/10\n",
            "50/50 [==============================] - 65s 1s/step - loss: 0.1161 - accuracy: 0.9750 - val_loss: 0.5577 - val_accuracy: 0.8000\n",
            "Epoch 5/10\n",
            "50/50 [==============================] - 63s 1s/step - loss: 0.0809 - accuracy: 0.9869 - val_loss: 0.5890 - val_accuracy: 0.8325\n",
            "Epoch 6/10\n",
            "50/50 [==============================] - 65s 1s/step - loss: 0.0811 - accuracy: 0.9844 - val_loss: 0.5276 - val_accuracy: 0.8275\n",
            "Epoch 7/10\n",
            "50/50 [==============================] - 65s 1s/step - loss: 0.0641 - accuracy: 0.9869 - val_loss: 0.5905 - val_accuracy: 0.8350\n",
            "Epoch 8/10\n",
            "50/50 [==============================] - 62s 1s/step - loss: 0.0621 - accuracy: 0.9869 - val_loss: 0.5165 - val_accuracy: 0.8350\n",
            "Epoch 9/10\n",
            "50/50 [==============================] - 61s 1s/step - loss: 0.0648 - accuracy: 0.9869 - val_loss: 0.5806 - val_accuracy: 0.8150\n",
            "Epoch 10/10\n",
            "50/50 [==============================] - 63s 1s/step - loss: 0.0550 - accuracy: 0.9850 - val_loss: 0.6432 - val_accuracy: 0.8075\n",
            "Test loss: 0.6432315707206726\n",
            "Test accuracy: 0.8075000047683716\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression"
      ],
      "metadata": {
        "id": "hrfCda0DNfPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Read in the dataset\n",
        "# df = pd.read_csv('p1_dataset.csv')\n",
        "df = new_df\n",
        "\n",
        "# Sample from each label, reducing the requested size if necessary\n",
        "sample_size = 10000\n",
        "if len(label_0_df) < sample_size:\n",
        "    sample_size = len(label_0_df)\n",
        "if len(label_1_df) < sample_size:\n",
        "    sample_size = len(label_1_df)\n",
        "\n",
        "label_0_df = label_0_df.sample(n=sample_size, random_state=42, replace=True)\n",
        "label_1_df = label_1_df.sample(n=sample_size, random_state=42, replace=True)\n",
        "\n",
        "\n",
        "# Combine the two samples\n",
        "sample_df = pd.concat([label_0_df, label_1_df])\n",
        "\n",
        "# Convert the text prompts to a bag-of-words representation\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(sample_df[['prompt1', 'prompt2']].apply(lambda x: ' '.join(x), axis=1))\n",
        "\n",
        "# Convert the labels to be in the range [0, 1]\n",
        "y = sample_df['class'].values\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model architecture\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model on test data\n",
        "score = model.score(X_test, y_test)\n",
        "print(f'Test accuracy: {score}')\n"
      ],
      "metadata": {
        "id": "3FHfgPbTNiUX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ea8060a-3919-461c-a700-e9a216fed0c2"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest"
      ],
      "metadata": {
        "id": "RGJTcZJJOghy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Read in the dataset\n",
        "# df = pd.read_csv('p1_dataset.csv')\n",
        "df = new_df\n",
        "\n",
        "# Sample from each label, reducing the requested size if necessary\n",
        "sample_size = 10000\n",
        "if len(label_0_df) < sample_size:\n",
        "    sample_size = len(label_0_df)\n",
        "if len(label_1_df) < sample_size:\n",
        "    sample_size = len(label_1_df)\n",
        "\n",
        "label_0_df = label_0_df.sample(n=sample_size, random_state=42, replace=True)\n",
        "label_1_df = label_1_df.sample(n=sample_size, random_state=42, replace=True)\n",
        "\n",
        "\n",
        "# Combine the two samples\n",
        "sample_df = pd.concat([label_0_df, label_1_df])\n",
        "\n",
        "# Convert the text prompts to a bag-of-words representation\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(sample_df[['prompt1', 'prompt2']].apply(lambda x: ' '.join(x), axis=1))\n",
        "\n",
        "# Convert the labels to be in the range [0, 1]\n",
        "y = sample_df['class'].values\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model architecture\n",
        "model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model on test data\n",
        "score = model.score(X_test, y_test)\n",
        "print(f'Test accuracy: {score}')\n"
      ],
      "metadata": {
        "id": "T5RaXCp6OuqH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a642b06-2c9f-4fc9-ff67-b0949884035b"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.93\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the trained model to classify new prompt pairs\n",
        "def classify_prompt_pair(prompt1, prompt2, model, vectorizer):\n",
        "    prompt = vectorizer.transform([' '.join([prompt1, prompt2])])\n",
        "    y_pred = model.predict(prompt)[0]\n",
        "    if y_pred == 0:\n",
        "        return 'generated by the same method'\n",
        "    else:\n",
        "        return 'generated by different methods'"
      ],
      "metadata": {
        "id": "J1ZIXANxPLTO"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt1 = \"she makes sure unwanted food gets to hungry americans cnn  the online tipping scheme employing the people to ask guests to pay would go a long way to help make everyday objects more accessible to the hungry american people according to a new study from the foodgift club at mercy college in louisville kentuckynconsumers surveyed by the organization food for those in need indicated that most the supporters recognized that a person has food in his or her pocket when they ask guests who pay a tip the only way possible according to the reportnfiftyseven percent of those surveyed 55 million people agreed that theres an urgent need for a communitywide initiative to assist millions of americans who do not have enough to eat in these hungry homesnthe survey was conducted in march by the food for those in need organization based at the university of louisvillenfood for those in need officials said in a statement that the organization plans to partner with the nonprofit organization to find a way to help poor americansnthe hungry citizens  the most in need of hunger assistance nationwide said evans follin who oversees the organizations marketing and distribution we know that americans are fed up and hunger is one of the top priorities for the federal government millions of americans are facing a minimum budget shortfall as a result of the current economic crisis with food insecurity mounting and the number of americans still unable to purchase basic goods such as small essentials such as milk eggs and produce risingnin the department of agricultures rural development and production program more than onethird of the population or an estimated 14 million people are food insecure or at least have insufficient enough to eat according to the reportnconsumers who talk about hunger around food and benefits themselves are more likely to identify the opportunity as food from a local deli a convenience store or the butcher shop according to the food for those in need reportnconsumers who recommend someone to donate food to the hungry through a friend or family member indicated they feel empowered to make the need a priority when ordering food those food donations usually include coupons or tokens they would like to pass along to others who receive them the report saidnhumanity may look to and accept a few givers as part of an effort to provide additional food to a large number of the hungry americans at our very own a communitys number one cause of hunger the study saidnother positive charitable behaviors for individuals and families need to be emphasized in this group and that is why the food for those in need initiative focuses on finding partnersn\"\n",
        "prompt2 = \"how budapest became a fine dining force to be reckoned with and you were the one who made it happen because of which was your special role in that regard why for what reason  why noteopeod i have heard all about this movie but never actually seen any footage from its production so as far we are concerned there is no evidence whatsoever on our part regarding anything being done by any other person or entity at least none directly connected within hungary nor outside thereof such people would certainly need to know more concerning their own activities if they really wanted us involved either here inside hungarian territory elsewhere than where now exist some sorta network involving various individuals both locally along side national level even further out into the international arena perhaps including foreign governments like those currently in power over iran syria libya egypt iraq afghanistan pakistan yemen somalia sudan tanzania kenya uganda rwanda burundi zaire etc whatever these people might want them to do then again after realizing how much damage has been caused already through just doing nothing could possibly justify continuing without action against anyone else yet another example thus far only one though many others may well arise later upon the realizations shown me throughout my life however until next time letll keep things simple enough between ourselves rather than making too complicated an issue since most likely nobody will care anyway regardless whether something happens otherwise besides myself personally when eventually everything comes together before everyone can finally see themselves clearly hence also the point behind keeping matters simple while still remaining open up instead having someone try hard trying to make sure everybody knows exactly whose fault each individual situation truly lies under whom ever heshe believes should take responsibility therefore giving away very little information beyond simply stating oneself responsible plus pointing fingers towards whoever does wrong once somebody realized himself first hand quite often times becomes impossible due mostly owing mainly down to the fact every situation changes considerably depending entirely solely based off personal opinion versus objective truth despite knowing better yourself right around today maybe tomorrow sometime soon thereafter although neither one shall always be true nonetheless whenever situations change sometimes difficult decisions must inevitably come forth especially considering certain factors given above namely the following ones first thing foremost please remember beforehand everyone doesnt deserve happiness except yourselveseop how did anybody get involved during filming itself unless specifically asked earlier according back home somewhere near budapest apparently few people seem to understand precisely how anyone got involved anywhere close sufficient quantity nevertheless thanks per usual wherever possible toward getting started properly beginning tonight hopefully shortly afterwards yes sir thankyou indeed yeah sir thanks thanks mister yes thank you mr thank you miss sorry miss sorry miss oh sorry miss oh sorry miss o sorry miss o too late miss late miss d dear d dear\"\n",
        "classification = classify_prompt_pair(prompt1, prompt2, model, vectorizer)\n",
        "print(f'{classification}.')"
      ],
      "metadata": {
        "id": "HRXkuu0IPsba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DistilBERT"
      ],
      "metadata": {
        "id": "qe8FNpBTDWYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# # Load the dataset\n",
        "# # df = pd.read_csv(\"p1_dataset.csv\")\n",
        "# df = new_df\n",
        "\n",
        "# # Split the dataset into training and testing sets\n",
        "# train_df, test_df = train_test_split(df, test_size=0.2, stratify=df[\"label\"])\n",
        "\n",
        "# # Sample 10000 rows with label 0 and 10000 rows with label 1\n",
        "# label_0 = train_df[train_df[\"label\"] == 0].sample(n=10000)\n",
        "# label_1 = train_df[train_df[\"label\"] == 1].sample(n=10000)\n",
        "\n",
        "# # Concatenate the sampled dataframes\n",
        "# train_df = pd.concat([label_0, label_1])\n",
        "\n",
        "# # Shuffle the training dataframe\n",
        "# train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# # Create two numpy arrays, one for prompt1 and one for prompt2\n",
        "# X_train = train_df[[\"prompt1\", \"prompt2\"]].to_numpy()\n",
        "# y_train = train_df[\"label\"].to_numpy()\n",
        "\n",
        "# X_test = test_df[[\"prompt1\", \"prompt2\"]].to_numpy()\n",
        "# y_test = test_df[\"label\"].to_numpy()\n",
        "\n",
        "# # Train the model and evaluate it on the test set\n",
        "# # You can use a pre-trained model like DistilBERT to classify the prompts\n",
        "# # Here's an example using TFDistilBertForSequenceClassification from the transformers library\n",
        "# from transformers import TFDistilBertForSequenceClassification, DistilBertTokenizer\n",
        "# import tensorflow as tf\n",
        "\n",
        "# # Load the pre-trained tokenizer and model\n",
        "# tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "# model = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "# # Tokenize the prompts\n",
        "# encoded_inputs = tokenizer(X_train.tolist(), padding=True, truncation=True, return_tensors=\"tf\")\n",
        "\n",
        "# # Convert the labels to a tensor\n",
        "# y_train_tensor = tf.convert_to_tensor(y_train)\n",
        "\n",
        "# # Train the model\n",
        "# model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "# model.fit(encoded_inputs, y_train_tensor, epochs=3)\n",
        "\n",
        "# # Tokenize the prompts in the test set\n",
        "# encoded_test_inputs = tokenizer(X_test.tolist(), padding=True, truncation=True, return_tensors=\"tf\")\n",
        "\n",
        "# # Evaluate the model on the test set\n",
        "# test_loss, test_acc = model.evaluate(encoded_test_inputs, tf.convert_to_tensor(y_test))\n",
        "# print(f\"Test loss: {test_loss}, Test accuracy: {test_acc}\")\n"
      ],
      "metadata": {
        "id": "JfK1wkXbdYRQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}