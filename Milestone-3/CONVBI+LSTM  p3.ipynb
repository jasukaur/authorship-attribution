{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyarBgQsOmvz",
        "outputId": "89838fe6-c49f-43cc-9c48-115e496a3712"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/6\n",
            "48/48 [==============================] - 30s 271ms/step - loss: 1.6432 - accuracy: 0.3784 - val_loss: 0.7797 - val_accuracy: 0.7232\n",
            "Epoch 2/6\n",
            "48/48 [==============================] - 12s 254ms/step - loss: 0.4283 - accuracy: 0.8593 - val_loss: 0.4362 - val_accuracy: 0.8401\n",
            "Epoch 3/6\n",
            "48/48 [==============================] - 11s 234ms/step - loss: 0.1394 - accuracy: 0.9624 - val_loss: 0.4175 - val_accuracy: 0.8616\n",
            "Epoch 4/6\n",
            "48/48 [==============================] - 12s 242ms/step - loss: 0.0533 - accuracy: 0.9871 - val_loss: 0.4695 - val_accuracy: 0.8525\n",
            "Epoch 5/6\n",
            "48/48 [==============================] - 10s 200ms/step - loss: 0.0256 - accuracy: 0.9941 - val_loss: 0.4482 - val_accuracy: 0.8695\n",
            "Epoch 6/6\n",
            "48/48 [==============================] - 11s 224ms/step - loss: 0.0191 - accuracy: 0.9966 - val_loss: 0.7453 - val_accuracy: 0.8277\n",
            "Epoch 1/5\n",
            "48/48 [==============================] - 18s 340ms/step - loss: 1.8880 - accuracy: 0.3120 - val_loss: 1.3706 - val_accuracy: 0.4785\n",
            "Epoch 2/5\n",
            "48/48 [==============================] - 11s 240ms/step - loss: 1.0322 - accuracy: 0.6238 - val_loss: 0.7172 - val_accuracy: 0.7363\n",
            "Epoch 3/5\n",
            "48/48 [==============================] - 11s 225ms/step - loss: 0.6540 - accuracy: 0.7762 - val_loss: 0.6236 - val_accuracy: 0.7689\n",
            "Epoch 4/5\n",
            "48/48 [==============================] - 11s 228ms/step - loss: 0.3632 - accuracy: 0.8934 - val_loss: 0.6046 - val_accuracy: 0.7872\n",
            "Epoch 5/5\n",
            "48/48 [==============================] - 10s 211ms/step - loss: 0.3532 - accuracy: 0.8908 - val_loss: 0.7486 - val_accuracy: 0.7631\n",
            "48/48 [==============================] - 1s 8ms/step\n",
            "48/48 [==============================] - 1s 8ms/step\n",
            "Combined model accuracy: 0.8426892950391645\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.96      0.92       227\n",
            "           1       0.99      1.00      0.99       222\n",
            "           2       0.83      0.54      0.65       224\n",
            "           3       0.58      0.96      0.72       205\n",
            "           4       0.98      0.99      0.98       202\n",
            "           5       0.99      0.84      0.91       205\n",
            "           6       0.83      0.73      0.78       205\n",
            "           7       0.82      0.33      0.47        42\n",
            "\n",
            "    accuracy                           0.84      1532\n",
            "   macro avg       0.86      0.79      0.80      1532\n",
            "weighted avg       0.87      0.84      0.84      1532\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Conv1D, MaxPooling1D, LSTM, Dropout, Flatten, Bidirectional\n",
        "from keras.utils import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "data = pd.read_csv(\"input.csv\")\n",
        "\n",
        "texts = data['text'].tolist()\n",
        "classes = data['class'].tolist()\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "tokenized_texts = [word_tokenize(text.lower()) for text in texts]\n",
        "stop_words = set(stopwords.words('english'))\n",
        "punctuation = set(string.punctuation)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "preprocessed_texts = []\n",
        "for tokens in tokenized_texts:\n",
        "    filtered_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words and token not in punctuation]\n",
        "    preprocessed_texts.append(filtered_tokens)\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(preprocessed_texts)\n",
        "word_index = tokenizer.word_index\n",
        "sequences = tokenizer.texts_to_sequences(preprocessed_texts)\n",
        "max_sequence_length = 500\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_classes = label_encoder.fit_transform(classes)\n",
        "num_classes = len(set(classes))\n",
        "one_hot_classes = to_categorical(encoded_classes, num_classes=num_classes)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, one_hot_classes, test_size=0.2, random_state=42)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(word_index) + 1, 100, input_length=max_sequence_length))\n",
        "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=5))\n",
        "model.add(Bidirectional(LSTM(128)))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=128)\n",
        "\n",
        "model2 = Sequential()\n",
        "model2.add(Embedding(len(word_index) + 1, 100, input_length=max_sequence_length))\n",
        "model2.add(LSTM(128))\n",
        "model2.add(Dropout(0.2))\n",
        "model2.add(Dense(num_classes, activation='softmax'))\n",
        "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model2.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=128)\n",
        "\n",
        "cnn_lstm_predictions = model.predict(X_test)\n",
        "lstm_rnn_predictions = model2.predict(X_test)\n",
        "\n",
        "combined_predictions = []\n",
        "for i in range(len(cnn_lstm_predictions)):\n",
        "    combined_predictions.append(np.argmax(cnn_lstm_predictions[i] + lstm_rnn_predictions[i]))\n",
        "\n",
        "combined_accuracy = accuracy_score(np.argmax(y_test, axis=1), combined_predictions)\n",
        "print(\"Combined model accuracy:\", combined_accuracy)\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(np.argmax(y_test, axis=1), combined_predictions))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qXO3jzc05k-h"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/charansaisadla/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/charansaisadla/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-05-09 12:55:35.644279: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "37/37 [==============================] - 28s 745ms/step - loss: 1.8806 - accuracy: 0.3483 - val_loss: 1.1191 - val_accuracy: 0.6053\n",
            "Epoch 2/5\n",
            "37/37 [==============================] - 29s 794ms/step - loss: 0.8174 - accuracy: 0.7130 - val_loss: 0.7992 - val_accuracy: 0.7221\n",
            "Epoch 3/5\n",
            "37/37 [==============================] - 29s 785ms/step - loss: 0.4261 - accuracy: 0.8592 - val_loss: 0.7758 - val_accuracy: 0.7361\n",
            "Epoch 4/5\n",
            "37/37 [==============================] - 29s 785ms/step - loss: 0.1786 - accuracy: 0.9478 - val_loss: 0.8523 - val_accuracy: 0.7327\n",
            "Epoch 5/5\n",
            "37/37 [==============================] - 29s 779ms/step - loss: 0.0675 - accuracy: 0.9856 - val_loss: 0.9687 - val_accuracy: 0.7421\n",
            "74/74 [==============================] - 3s 41ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.94      0.93       214\n",
            "           1       0.50      0.78      0.61       209\n",
            "           2       1.00      1.00      1.00       217\n",
            "           3       0.74      0.37      0.49       213\n",
            "           4       0.55      0.46      0.50       217\n",
            "           5       0.49      0.60      0.54       222\n",
            "           6       0.87      0.66      0.75       220\n",
            "           7       0.69      0.66      0.67       202\n",
            "           8       0.67      0.81      0.73       206\n",
            "           9       0.99      0.95      0.97       215\n",
            "          10       0.92      0.96      0.94       211\n",
            "\n",
            "    accuracy                           0.74      2346\n",
            "   macro avg       0.76      0.74      0.74      2346\n",
            "weighted avg       0.76      0.74      0.74      2346\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Conv1D, MaxPooling1D, LSTM, Dropout, Flatten, Bidirectional\n",
        "from keras.utils import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "\n",
        "# Step 1: Load the data from the CSV file\n",
        "\n",
        "data = pd.read_csv(\"/Users/charansaisadla/Downloads/input-2.csv\")\n",
        "\n",
        "texts = data['text'].tolist()\n",
        "classes = data['class'].tolist()\n",
        "\n",
        "# Step 2: Convert the text data into numerical representations\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "tokenized_texts = []\n",
        "for text in texts:\n",
        "    tokens = [lemmatizer.lemmatize(token.lower()) for token in word_tokenize(text) if\n",
        "              token.lower() not in stop_words and token.lower() not in string.punctuation]\n",
        "    tokenized_texts.append(tokens)\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(tokenized_texts)\n",
        "word_index = tokenizer.word_index\n",
        "sequences = tokenizer.texts_to_sequences(tokenized_texts)\n",
        "max_sequence_length = 500\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
        "\n",
        "# Step 3: Encode the class labels\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_classes = label_encoder.fit_transform(classes)\n",
        "num_classes = len(label_encoder.classes_)\n",
        "one_hot_classes = to_categorical(encoded_classes, num_classes=num_classes)\n",
        "\n",
        "# Step 4: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, one_hot_classes, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: Define and train the CRNN model\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(word_index) + 1, 100, input_length=max_sequence_length))\n",
        "model.add(Conv1D(256, 8, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=5))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=256)\n",
        "\n",
        "# Step 6: Evaluate the model on the test set\n",
        "y_pred_prob = model.predict(X_test)\n",
        "y_pred = y_pred_prob.argmax(axis=1)\n",
        "y_test_encoded = y_test.argmax(axis=1)\n",
        "report = classification_report(y_test_encoded, y_pred)\n",
        "print(report)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
