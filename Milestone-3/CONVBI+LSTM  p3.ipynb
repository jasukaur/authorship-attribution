{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Conv1D, MaxPooling1D, LSTM, Dropout, Flatten, Bidirectional\n",
        "from keras.utils import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "data = pd.read_csv(\"input.csv\")\n",
        "\n",
        "texts = data['text'].tolist()\n",
        "classes = data['class'].tolist()\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "tokenized_texts = [word_tokenize(text.lower()) for text in texts]\n",
        "stop_words = set(stopwords.words('english'))\n",
        "punctuation = set(string.punctuation)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "preprocessed_texts = []\n",
        "for tokens in tokenized_texts:\n",
        "    filtered_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words and token not in punctuation]\n",
        "    preprocessed_texts.append(filtered_tokens)\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(preprocessed_texts)\n",
        "word_index = tokenizer.word_index\n",
        "sequences = tokenizer.texts_to_sequences(preprocessed_texts)\n",
        "max_sequence_length = 500\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_classes = label_encoder.fit_transform(classes)\n",
        "num_classes = len(set(classes))\n",
        "one_hot_classes = to_categorical(encoded_classes, num_classes=num_classes)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, one_hot_classes, test_size=0.2, random_state=42)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(word_index) + 1, 100, input_length=max_sequence_length))\n",
        "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=5))\n",
        "model.add(Bidirectional(LSTM(128)))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=128)\n",
        "\n",
        "model2 = Sequential()\n",
        "model2.add(Embedding(len(word_index) + 1, 100, input_length=max_sequence_length))\n",
        "model2.add(LSTM(128))\n",
        "model2.add(Dropout(0.2))\n",
        "model2.add(Dense(num_classes, activation='softmax'))\n",
        "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model2.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=128)\n",
        "\n",
        "cnn_lstm_predictions = model.predict(X_test)\n",
        "lstm_rnn_predictions = model2.predict(X_test)\n",
        "\n",
        "combined_predictions = []\n",
        "for i in range(len(cnn_lstm_predictions)):\n",
        "    combined_predictions.append(np.argmax(cnn_lstm_predictions[i] + lstm_rnn_predictions[i]))\n",
        "\n",
        "combined_accuracy = accuracy_score(np.argmax(y_test, axis=1), combined_predictions)\n",
        "print(\"Combined model accuracy:\", combined_accuracy)\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(np.argmax(y_test, axis=1), combined_predictions))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyarBgQsOmvz",
        "outputId": "89838fe6-c49f-43cc-9c48-115e496a3712"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n",
            "48/48 [==============================] - 30s 271ms/step - loss: 1.6432 - accuracy: 0.3784 - val_loss: 0.7797 - val_accuracy: 0.7232\n",
            "Epoch 2/6\n",
            "48/48 [==============================] - 12s 254ms/step - loss: 0.4283 - accuracy: 0.8593 - val_loss: 0.4362 - val_accuracy: 0.8401\n",
            "Epoch 3/6\n",
            "48/48 [==============================] - 11s 234ms/step - loss: 0.1394 - accuracy: 0.9624 - val_loss: 0.4175 - val_accuracy: 0.8616\n",
            "Epoch 4/6\n",
            "48/48 [==============================] - 12s 242ms/step - loss: 0.0533 - accuracy: 0.9871 - val_loss: 0.4695 - val_accuracy: 0.8525\n",
            "Epoch 5/6\n",
            "48/48 [==============================] - 10s 200ms/step - loss: 0.0256 - accuracy: 0.9941 - val_loss: 0.4482 - val_accuracy: 0.8695\n",
            "Epoch 6/6\n",
            "48/48 [==============================] - 11s 224ms/step - loss: 0.0191 - accuracy: 0.9966 - val_loss: 0.7453 - val_accuracy: 0.8277\n",
            "Epoch 1/5\n",
            "48/48 [==============================] - 18s 340ms/step - loss: 1.8880 - accuracy: 0.3120 - val_loss: 1.3706 - val_accuracy: 0.4785\n",
            "Epoch 2/5\n",
            "48/48 [==============================] - 11s 240ms/step - loss: 1.0322 - accuracy: 0.6238 - val_loss: 0.7172 - val_accuracy: 0.7363\n",
            "Epoch 3/5\n",
            "48/48 [==============================] - 11s 225ms/step - loss: 0.6540 - accuracy: 0.7762 - val_loss: 0.6236 - val_accuracy: 0.7689\n",
            "Epoch 4/5\n",
            "48/48 [==============================] - 11s 228ms/step - loss: 0.3632 - accuracy: 0.8934 - val_loss: 0.6046 - val_accuracy: 0.7872\n",
            "Epoch 5/5\n",
            "48/48 [==============================] - 10s 211ms/step - loss: 0.3532 - accuracy: 0.8908 - val_loss: 0.7486 - val_accuracy: 0.7631\n",
            "48/48 [==============================] - 1s 8ms/step\n",
            "48/48 [==============================] - 1s 8ms/step\n",
            "Combined model accuracy: 0.8426892950391645\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.96      0.92       227\n",
            "           1       0.99      1.00      0.99       222\n",
            "           2       0.83      0.54      0.65       224\n",
            "           3       0.58      0.96      0.72       205\n",
            "           4       0.98      0.99      0.98       202\n",
            "           5       0.99      0.84      0.91       205\n",
            "           6       0.83      0.73      0.78       205\n",
            "           7       0.82      0.33      0.47        42\n",
            "\n",
            "    accuracy                           0.84      1532\n",
            "   macro avg       0.86      0.79      0.80      1532\n",
            "weighted avg       0.87      0.84      0.84      1532\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qXO3jzc05k-h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}