# Authorship Attribution for Neural Text Generation (Synthetically generated text detection)

Advancements in neural language modeling have made it challenging to differentiate between human and machine-generated texts. However, distinguishing between the two has become crucial for tasks like Authorship Attribution, where identifying the origin of a text is essential. So in our implementation, we have 3 problems:
1. Determine if texts T1 and T2 are generated by the same NLG method or human.
2. Differentiate between a text T1 written by a human or generated by any of k NLG methods.
3. Attribute the text T1 to a specific NLG method among k alternatives.

# Data Source

In the Milestone 2 folder, you can find the "gpt3extraction.py" script, which was utilized to scrape data from OpenAI's GPT-3 and InstructGPT models. To extract data for GPT-3 and InstructGPT, you need to modify the name of the model-engine in the script. The "prompts.csv" file was used as input for this script. Additionally, the "input.csv" file, which has been cleaned and concatenated, is present in the data directory and is used as input for various model executions.

To extract data from Reddit, use the "reddit-extraction.ipynb" script in the Milestone2 directory. Modify the subreddit name in the script for each subreddit you want to scrape. The cleaned and concatenated data can be found in the "reddit-input.csv" file in the Reddit subdirectory of the data folder.


# Setting up instructions

To execute the notebooks and files on your local machine and visualize the data, you need to ensure that all the required dependencies are installed. Here are the steps to achieve this:
1. Open a command prompt or terminal.
2. Navigate to the directory where the requirements.txt file is located. You can use the cd command to change directories.
3. Once you are in the correct directory, execute the following command to install all the required dependencies:

> pip install -r requirements.txt


To successfully execute the code, please follow these steps:
1. Modify the data path in the code to match the location of your data file (e.g., input.csv, reddit_input.csv) on your local drive or preferred storage location.
2. If you are using Google Colab, change the runtime to 'GPU' for better performance. (Google Colab is recommended)
3. Ensure that your drive is mounted in Google Colab if you are accessing data from Google Drive.
4. Navigate to the "Runtime" menu and select "Run all" to execute all code cells in the notebook.

# Folder and Files descriptions

1. The Milestone1 directory contains the PowerPoint presentation that was delivered during Milestone 1.

2. The Milestone2 directory consists of several components, including scripts for extracting data from OpenAI's GPT-3 and InstructGPT models based on provided prompts. Additionally, there is a script available for scraping data from Reddit. Moreover, the directory contains Python notebooks for P1, P2, and P3, as defined in the baseline architecture.

3. The Milestone3 directory contains various tried and tested models for solving the defined problems. Specifically:

- For P1, the Siamese Neural Network code can be found in the P1-Siamese.ipynb file.
- For P2, the BERT Model code is implemented in the P2-BERT.ipynb file.
- For P3, the Roberta Model code is provided in the P3-multiple-roberta.ipynb file. Additionally, the directory contains other notebooks that explore alternative models that did not meet the desired evaluation metrics and thus were not selected as the final models.

4. The data directory contains various input files used for data cleaning and preprocessing. It also includes the cleaned and concatenated input files that were utilized by the models. Additionally, within the Reddit subdirectory, you can locate all the Reddit data related to the project. 

<br>

**THANK YOU**