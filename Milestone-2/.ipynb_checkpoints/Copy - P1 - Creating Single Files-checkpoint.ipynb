{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/arsh/Jasleen/Spring 2023/NLP/Group Project/Authorship-Attribution-for-Neural-Text-Generation-master/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrl = pd.read_csv(data_path + 'new_ctrl.csv')\n",
    "gpt = pd.read_csv(data_path + 'new_gpt.csv')\n",
    "gpt2 = pd.read_csv(data_path + 'new_gpt2.csv')\n",
    "\n",
    "grover = pd.read_csv(data_path + 'new_grover.csv')\n",
    "xlm = pd.read_csv(data_path + 'new_xlm.csv')\n",
    "xlnet = pd.read_csv(data_path + 'new_xlnet.csv')\n",
    "\n",
    "pplm = pd.read_csv(data_path + 'new_pplm.csv')\n",
    "human = pd.read_csv(data_path + 'new_human.csv')\n",
    "fair = pd.read_csv(data_path + 'new_fair.csv')\n",
    "\n",
    "gpt3=pd.read_csv(data_path +\"new_gpt3.csv\")\n",
    "instructgpt=pd.read_csv(data_path +\"new_instructgpt.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Generation', 'label'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctrl.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_binary_t2 = pd.concat([human.sample(1066), ctrl.sample(1066), gpt.sample(1066), gpt2.sample(1066), gpt3.sample(1066), instructgpt.sample(1066), grover.sample(1066),\n",
    "xlm.sample(1066), xlnet.sample(1066), pplm.sample(1066), fair.sample(1066), ctrl.sample(1066),\n",
    "gpt.sample(1066), gpt2.sample(1066), gpt3.sample(1066), instructgpt.sample(1066), grover.sample(1066), xlm.sample(1066), xlnet.sample(1066),\n",
    "pplm.sample(1066), fair.sample(1066), human.sample(1066), ctrl.sample(1066), human.sample(1066),\n",
    "gpt2.sample(1066), gpt.sample(1066), gpt3.sample(1066), instructgpt.sample(1066), grover.sample(1066), xlm.sample(1066), xlnet.sample(1066),\n",
    "pplm.sample(1066), fair.sample(1066), human.sample(1066), ctrl.sample(1066), gpt.sample(1066),\n",
    "gpt2.sample(1066), gpt3.sample(1066), instructgpt.sample(1066), grover.sample(1066), xlm.sample(1066), xlnet.sample(1066), pplm.sample(1066),\n",
    "fair.sample(1066), human.sample(1066), ctrl.sample(1066), gpt.sample(1066), gpt2.sample(1066), gpt3.sample(1066),\n",
    "instructgpt.sample(1066), grover.sample(1066), xlm.sample(1066), xlnet.sample(1066), pplm.sample(1066), fair.sample(1066),\n",
    "human.sample(1066), ctrl.sample(1066), gpt.sample(1066), gpt2.sample(1066), gpt3.sample(1066), instructgpt.sample(1066),\n",
    "grover.sample(1066), xlm.sample(1066), xlnet.sample(1066), pplm.sample(1066), fair.sample(1066), human.sample(1066),\n",
    "ctrl.sample(1066), gpt.sample(1066), gpt2.sample(1066), gpt3.sample(1066), instructgpt.sample(1066), grover.sample(1066),\n",
    "xlm.sample(1066), xlnet.sample(1066), pplm.sample(1066), fair.sample(1066), human.sample(1066), ctrl.sample(1066),\n",
    "gpt.sample(1066), gpt2.sample(1066), gpt3.sample(1066), instructgpt.sample(1066), grover.sample(1066), xlm.sample(1066),\n",
    "xlnet.sample(1066), pplm.sample(1066), fair.sample(1066), human.sample(1066), ctrl.sample(1066), gpt.sample(1066),\n",
    "gpt2.sample(1066), gpt3.sample(1066), instructgpt.sample(1066), grover.sample(1066), xlm.sample(1066), xlnet.sample(1066),\n",
    "pplm.sample(1066), fair.sample(1066)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_binary_t1 = pd.concat([human,human, human, human, human, human, human, human, human, ctrl, ctrl, ctrl, ctrl, ctrl, ctrl,\n",
    "ctrl, ctrl, ctrl, gpt, gpt, gpt, gpt, gpt, gpt, gpt, gpt, gpt, gpt2, gpt2, gpt2, gpt2, gpt2, gpt2,\n",
    "gpt2, gpt2, gpt2, grover, grover, grover, grover, grover, grover, grover, grover, grover, xlm, xlm,\n",
    "xlm, xlm, xlm, xlm, xlm, xlm, xlm, xlnet, xlnet, xlnet, xlnet, xlnet, xlnet, xlnet, xlnet, xlnet,\n",
    "pplm, pplm, pplm, pplm, pplm, pplm, pplm, pplm, pplm, fair, fair, fair, fair, fair, fair, fair, fair, fair,\n",
    "gpt3,gpt3,gpt3,gpt3,gpt3,gpt3,gpt3,gpt3,gpt3, instructgpt,instructgpt,instructgpt,instructgpt,instructgpt,instructgpt,instructgpt,instructgpt,instructgpt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(random_binary_t1) == len(random_binary_t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105534"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(random_binary_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Generation</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>398</td>\n",
       "      <td>transcripts transcripts return to transcripts ...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>625</td>\n",
       "      <td>rising seas are turning miamis high ground int...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>685</td>\n",
       "      <td>warren buffett has the right answer to crony c...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961</th>\n",
       "      <td>961</td>\n",
       "      <td>bbc news bbc news two west yorkshire parades h...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>575</td>\n",
       "      <td>trumps positions on fisa and coronavirus respo...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                         Generation  label\n",
       "398         398  transcripts transcripts return to transcripts ...  human\n",
       "625         625  rising seas are turning miamis high ground int...  human\n",
       "685         685  warren buffett has the right answer to crony c...  human\n",
       "961         961  bbc news bbc news two west yorkshire parades h...  human\n",
       "575         575  trumps positions on fisa and coronavirus respo...  human"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_binary_t2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_label = [np.ones(len(human)), \n",
    "             np.zeros(8 * 1066), \n",
    "             np.ones(len(ctrl)), \n",
    "             np.zeros(7 * 1066), \n",
    "             np.ones(len(gpt)), \n",
    "             np.zeros(7 * 1066),\n",
    "             np.ones(len(gpt2)),\n",
    "             np.zeros(7 * 1066),\n",
    "             np.ones(len(gpt3)),\n",
    "             np.zeros(7 * 1066),\n",
    "             np.ones(len(grover)),\n",
    "             np.zeros(7 * 1066),\n",
    "             np.ones(len(xlm)),\n",
    "             np.zeros(7 * 1066),\n",
    "             np.ones(len(xlnet)),\n",
    "             np.zeros(7 * 1066),\n",
    "             np.ones(len(pplm)),\n",
    "             np.zeros(7 * 1066),\n",
    "             np.ones(len(fair)),\n",
    "             np.zeros(7 * 1066),\n",
    "             np.ones(len(instructgpt)),\n",
    "             np.zeros(8 * 1066),\n",
    "             np.ones(len(gpt3)),\n",
    "             np.zeros(8 * 1066)\n",
    "            ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = [item for sublist in new_label for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105534"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105534"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(random_binary_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105534"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(random_binary_t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_random = pd.DataFrame({'T1': list(random_binary_t1['Generation']), 'T2': list(random_binary_t2['Generation']), 'class': flat_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_random.to_csv('binary_random.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T1</th>\n",
       "      <th>T2</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>latest headlines on cnn business latest headli...</td>\n",
       "      <td>transcripts transcripts return to transcripts ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>china wants to take a victory lap over its han...</td>\n",
       "      <td>rising seas are turning miamis high ground int...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>coronavirus disinformation creates challenges ...</td>\n",
       "      <td>warren buffett has the right answer to crony c...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>china coronavirus eating wild animals made ill...</td>\n",
       "      <td>bbc news bbc news two west yorkshire parades h...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chinas economy could shrink for the first time...</td>\n",
       "      <td>trumps positions on fisa and coronavirus respo...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105529</th>\n",
       "      <td>how much of your body is your own how much of ...</td>\n",
       "      <td>naomi campbell wears hazmat suit to airport am...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105530</th>\n",
       "      <td>how do you keep a space station clean how do y...</td>\n",
       "      <td>conservative she was not a faceless liberal sh...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105531</th>\n",
       "      <td>the city where you pay a years rent up front t...</td>\n",
       "      <td>eleonore laloux is looking to prove it as fran...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105532</th>\n",
       "      <td>the bbc news app gives you the best of bbc new...</td>\n",
       "      <td>heres what harvey weinstein said in court befo...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105533</th>\n",
       "      <td>learn how the bbc is working to strengthen tru...</td>\n",
       "      <td>ski at austrias soelden resort costs 234 more ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105534 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       T1  \\\n",
       "0       latest headlines on cnn business latest headli...   \n",
       "1       china wants to take a victory lap over its han...   \n",
       "2       coronavirus disinformation creates challenges ...   \n",
       "3       china coronavirus eating wild animals made ill...   \n",
       "4       chinas economy could shrink for the first time...   \n",
       "...                                                   ...   \n",
       "105529  how much of your body is your own how much of ...   \n",
       "105530  how do you keep a space station clean how do y...   \n",
       "105531  the city where you pay a years rent up front t...   \n",
       "105532  the bbc news app gives you the best of bbc new...   \n",
       "105533  learn how the bbc is working to strengthen tru...   \n",
       "\n",
       "                                                       T2  class  \n",
       "0       transcripts transcripts return to transcripts ...    1.0  \n",
       "1       rising seas are turning miamis high ground int...    1.0  \n",
       "2       warren buffett has the right answer to crony c...    1.0  \n",
       "3       bbc news bbc news two west yorkshire parades h...    1.0  \n",
       "4       trumps positions on fisa and coronavirus respo...    1.0  \n",
       "...                                                   ...    ...  \n",
       "105529  naomi campbell wears hazmat suit to airport am...    0.0  \n",
       "105530  conservative she was not a faceless liberal sh...    0.0  \n",
       "105531  eleonore laloux is looking to prove it as fran...    0.0  \n",
       "105532  heres what harvey weinstein said in court befo...    0.0  \n",
       "105533  ski at austrias soelden resort costs 234 more ...    0.0  \n",
       "\n",
       "[105534 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Balanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "balance_T1 = pd.concat([human.sample(1066), ctrl.sample(133), pplm.sample(133), gpt.sample(134), gpt2.sample(133), grover.sample(134), \n",
    "                     xlm.sample(133), xlnet.sample(133), fair.sample(133), gpt3.sample(133), instructgpt.sample(133),\n",
    "                       ctrl.sample(133), pplm.sample(133), gpt.sample(134), gpt2.sample(133), grover.sample(134), \n",
    "                     xlm.sample(133), xlnet.sample(133), fair.sample(133),gpt3.sample(133), instructgpt.sample(133), human.sample(1066)])\n",
    "balance_T2 = pd.concat([human, human, ctrl.sample(133), pplm.sample(133), gpt.sample(134), gpt2.sample(133), grover.sample(134), \n",
    "                     xlm.sample(133), xlnet.sample(133), fair.sample(133), gpt3.sample(133), instructgpt.sample(133),ctrl.sample(133), pplm.sample(133), gpt.sample(134), gpt2.sample(133), grover.sample(134), \n",
    "                     xlm.sample(133), xlnet.sample(133), fair.sample(133),gpt3.sample(133), instructgpt.sample(133)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Generation</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>latest headlines on cnn business latest headli...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>china wants to take a victory lap over its han...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>coronavirus disinformation creates challenges ...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>china coronavirus eating wild animals made ill...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>chinas economy could shrink for the first time...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>681</td>\n",
       "      <td>it would be a disaster for us to have to choos...</td>\n",
       "      <td>instructgpt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>390</td>\n",
       "      <td>bernie sanders plans to stay in the 2020 presi...</td>\n",
       "      <td>instructgpt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>578</td>\n",
       "      <td>trump grows more irate as his attempts to cont...</td>\n",
       "      <td>instructgpt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>484</td>\n",
       "      <td>brazil lashed by heavy rains leaving at least ...</td>\n",
       "      <td>instructgpt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>with society shutting down will tokyo 2020 go ...</td>\n",
       "      <td>instructgpt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4796 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                         Generation  \\\n",
       "0             0  latest headlines on cnn business latest headli...   \n",
       "1             1  china wants to take a victory lap over its han...   \n",
       "2             2  coronavirus disinformation creates challenges ...   \n",
       "3             3  china coronavirus eating wild animals made ill...   \n",
       "4             4  chinas economy could shrink for the first time...   \n",
       "..          ...                                                ...   \n",
       "681         681  it would be a disaster for us to have to choos...   \n",
       "390         390  bernie sanders plans to stay in the 2020 presi...   \n",
       "578         578  trump grows more irate as his attempts to cont...   \n",
       "484         484  brazil lashed by heavy rains leaving at least ...   \n",
       "21           21  with society shutting down will tokyo 2020 go ...   \n",
       "\n",
       "           label  \n",
       "0          human  \n",
       "1          human  \n",
       "2          human  \n",
       "3          human  \n",
       "4          human  \n",
       "..           ...  \n",
       "681  instructgpt  \n",
       "390  instructgpt  \n",
       "578  instructgpt  \n",
       "484  instructgpt  \n",
       "21   instructgpt  \n",
       "\n",
       "[4796 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balance_T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "balance_label = [np.ones(1066),\n",
    "                 np.zeros(1066), \n",
    "                 np.ones(1066), \n",
    "                np.zeros(1066),\n",
    "                ]\n",
    "\n",
    "\n",
    "# balance_label = [1 if i < 2398 else 0 for i in range(4796)]\n",
    "\n",
    "# balance_label = np.concatenate([np.ones(1066), np.zeros(1066), np.ones(1066), np.zeros(1066)])\n",
    "\n",
    "balance_label = [np.ones(1066),np.zeros(1066),np.ones(1066), np.zeros(1066),]\n",
    "balance_label = [item for sublist in balance_label for item in sublist] * 4\n",
    "balance_label = balance_label[:4796]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balance_label = [item for sublist in balance_label for item in sublist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4796"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(balance_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_p1 = pd.DataFrame({'T1': list(balance_T1['Generation']), 'T2': list(balance_T2['Generation']), 'class': balance_label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_p1.to_csv('balanced_p1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T1</th>\n",
       "      <th>T2</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>with coronavirus closing schools heres how you...</td>\n",
       "      <td>latest headlines on cnn business latest headli...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>my mother was raped in a nursing home at 88 my...</td>\n",
       "      <td>china wants to take a victory lap over its han...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chinese restaurants are losing business over c...</td>\n",
       "      <td>coronavirus disinformation creates challenges ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pornhub offers quarantined italians free acces...</td>\n",
       "      <td>china coronavirus eating wild animals made ill...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30 of vietnams most beautiful places 30 of vie...</td>\n",
       "      <td>chinas economy could shrink for the first time...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4791</th>\n",
       "      <td>pritzker prize 2020 irish duos win marks rare ...</td>\n",
       "      <td>it would be a disaster for us to have to choos...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4792</th>\n",
       "      <td>yearlong resolutions dont work heres how to ma...</td>\n",
       "      <td>bernie sanders plans to stay in the 2020 presi...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4793</th>\n",
       "      <td>photographer martin parr on britain in the age...</td>\n",
       "      <td>trump grows more irate as his attempts to cont...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4794</th>\n",
       "      <td>what the us can learn from singapores coronavi...</td>\n",
       "      <td>brazil lashed by heavy rains leaving at least ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4795</th>\n",
       "      <td>olafur eliasson on what art can do to fight cl...</td>\n",
       "      <td>with society shutting down will tokyo 2020 go ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4796 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     T1  \\\n",
       "0     with coronavirus closing schools heres how you...   \n",
       "1     my mother was raped in a nursing home at 88 my...   \n",
       "2     chinese restaurants are losing business over c...   \n",
       "3     pornhub offers quarantined italians free acces...   \n",
       "4     30 of vietnams most beautiful places 30 of vie...   \n",
       "...                                                 ...   \n",
       "4791  pritzker prize 2020 irish duos win marks rare ...   \n",
       "4792  yearlong resolutions dont work heres how to ma...   \n",
       "4793  photographer martin parr on britain in the age...   \n",
       "4794  what the us can learn from singapores coronavi...   \n",
       "4795  olafur eliasson on what art can do to fight cl...   \n",
       "\n",
       "                                                     T2  class  \n",
       "0     latest headlines on cnn business latest headli...    1.0  \n",
       "1     china wants to take a victory lap over its han...    1.0  \n",
       "2     coronavirus disinformation creates challenges ...    1.0  \n",
       "3     china coronavirus eating wild animals made ill...    1.0  \n",
       "4     chinas economy could shrink for the first time...    1.0  \n",
       "...                                                 ...    ...  \n",
       "4791  it would be a disaster for us to have to choos...    1.0  \n",
       "4792  bernie sanders plans to stay in the 2020 presi...    1.0  \n",
       "4793  trump grows more irate as his attempts to cont...    1.0  \n",
       "4794  brazil lashed by heavy rains leaving at least ...    1.0  \n",
       "4795  with society shutting down will tokyo 2020 go ...    1.0  \n",
       "\n",
       "[4796 rows x 3 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_p1 = balanced_p1.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6447916666666667\n"
     ]
    }
   ],
   "source": [
    "# X = balance_T1['Generation']\n",
    "# y = balance_T1['label']\n",
    "\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Create a CountVectorizer object\n",
    "# vectorizer = CountVectorizer()\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Fit the vectorizer to the training data and transform it into vectors of word frequencies\n",
    "# vectorizer.fit(X_train)\n",
    "# freq_train = vectorizer.transform(X_train)\n",
    "# freq_test = vectorizer.transform(X_test)\n",
    "\n",
    "# # Train the Naive Bayes classifier on the training data\n",
    "# nb = MultinomialNB()\n",
    "# nb.fit(freq_train, y_train)\n",
    "\n",
    "# # Make predictions on the testing data\n",
    "# y_pred = nb.predict(freq_test)\n",
    "\n",
    "# # Calculate the accuracy of the classifier\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.48854166666666665\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# # Create a TfidfVectorizer object\n",
    "# tfidf = TfidfVectorizer()\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Fit the vectorizer to the training data and transform it into vectors of tfidf values\n",
    "# tfidf.fit(X_train)\n",
    "# tfidf_train = tfidf.transform(X_train)\n",
    "# tfidf_test = tfidf.transform(X_test)\n",
    "\n",
    "# # Train the Naive Bayes classifier on the training data\n",
    "# nb = MultinomialNB()\n",
    "# nb.fit(tfidf_train, y_train)\n",
    "\n",
    "# # Make predictions on the testing data\n",
    "# y_pred = nb.predict(tfidf_test)\n",
    "\n",
    "# # Calculate the accuracy of the classifier\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8041666666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ctrl       1.00      1.00      1.00        57\n",
      "        fair       0.68      0.45      0.54        55\n",
      "         gpt       1.00      1.00      1.00        56\n",
      "        gpt2       0.73      0.61      0.67        49\n",
      "        gpt3       0.50      0.25      0.34        51\n",
      "      grover       0.58      0.15      0.24        47\n",
      "       human       0.77      0.98      0.86       455\n",
      " instructgpt       0.59      0.30      0.40        54\n",
      "        pplm       0.97      0.70      0.82        44\n",
      "         xlm       1.00      1.00      1.00        49\n",
      "       xlnet       1.00      1.00      1.00        43\n",
      "\n",
      "    accuracy                           0.80       960\n",
      "   macro avg       0.80      0.68      0.71       960\n",
      "weighted avg       0.79      0.80      0.78       960\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# X = balance_T1['Generation']\n",
    "# y = balance_T1['label']\n",
    "\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.svm import LinearSVC\n",
    "# from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# # Create a TfidfVectorizer object\n",
    "# vectorizer = TfidfVectorizer()\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Fit the vectorizer to the training data and transform it into vectors of TF-IDF values\n",
    "# vectorizer.fit(X_train)\n",
    "# tfidf_train = vectorizer.transform(X_train)\n",
    "# tfidf_test = vectorizer.transform(X_test)\n",
    "\n",
    "# # Train the SVM classifier on the training data\n",
    "# svm = LinearSVC()\n",
    "# svm.fit(tfidf_train, y_train)\n",
    "\n",
    "# # Make predictions on the testing data\n",
    "# y_pred = svm.predict(tfidf_test)\n",
    "\n",
    "# # Calculate the accuracy of the classifier\n",
    "# # Calculate the accuracy, precision, recall, and f1-score of the classifier\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(\"Accuracy:\", accuracy)\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6364583333333333\n"
     ]
    }
   ],
   "source": [
    "# X_i = balance_T2['Generation']\n",
    "# y_i = balance_T2['label']\n",
    "\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Create a CountVectorizer object\n",
    "# vectorizer = CountVectorizer()\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train_i, X_test_i, y_train_i, y_test_i = train_test_split(X_i, y_i, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Fit the vectorizer to the training data and transform it into vectors of word frequencies\n",
    "# vectorizer.fit(X_train_i)\n",
    "# freq_train_i = vectorizer.transform(X_train_i)\n",
    "# freq_test_i = vectorizer.transform(X_test_i)\n",
    "\n",
    "# # Train the Naive Bayes classifier on the training data\n",
    "# nb = MultinomialNB()\n",
    "# nb.fit(freq_train_i, y_train_i)\n",
    "\n",
    "# # Make predictions on the testing data\n",
    "# y_pred_i = nb.predict(freq_test_i)\n",
    "\n",
    "# # Calculate the accuracy of the classifier\n",
    "# accuracy = accuracy_score(y_test_i, y_pred_i)\n",
    "# print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4708333333333333\n"
     ]
    }
   ],
   "source": [
    "# X_i = balance_T2['Generation']\n",
    "# y_i = balance_T2['label']\n",
    "\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Create a TF-IDF vectorizer object\n",
    "# vectorizer = TfidfVectorizer()\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train_i, X_test_i, y_train_i, y_test_i = train_test_split(X_i, y_i, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Fit the vectorizer to the training data and transform it into vectors of word frequencies\n",
    "# vectorizer.fit(X_train_i)\n",
    "# freq_train_i = vectorizer.transform(X_train_i)\n",
    "# freq_test_i = vectorizer.transform(X_test_i)\n",
    "\n",
    "# # Train the Naive Bayes classifier on the training data\n",
    "# nb = MultinomialNB()\n",
    "# nb.fit(freq_train_i, y_train_i)\n",
    "\n",
    "# # Make predictions on the testing data\n",
    "# y_pred_i = nb.predict(freq_test_i)\n",
    "\n",
    "# # Calculate the accuracy of the classifier\n",
    "# accuracy = accuracy_score(y_test_i, y_pred_i)\n",
    "# print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8020833333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ctrl       1.00      1.00      1.00        51\n",
      "        fair       0.75      0.54      0.63        61\n",
      "         gpt       1.00      1.00      1.00        57\n",
      "        gpt2       0.68      0.52      0.59        44\n",
      "        gpt3       0.59      0.40      0.48        40\n",
      "      grover       0.43      0.11      0.17        55\n",
      "       human       0.76      0.98      0.86       427\n",
      " instructgpt       0.78      0.42      0.55        69\n",
      "        pplm       0.86      0.70      0.78        54\n",
      "         xlm       1.00      0.98      0.99        51\n",
      "       xlnet       0.98      0.94      0.96        51\n",
      "\n",
      "    accuracy                           0.80       960\n",
      "   macro avg       0.80      0.69      0.73       960\n",
      "weighted avg       0.79      0.80      0.78       960\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# X_i = balance_T2['Generation']\n",
    "# y_i = balance_T2['label']\n",
    "\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.svm import LinearSVC\n",
    "# from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# # Create a TF-IDF vectorizer object\n",
    "# vectorizer = TfidfVectorizer()\n",
    "\n",
    "# # # Split the data into training and testing sets\n",
    "# X_train_i, X_test_i, y_train_i, y_test_i = train_test_split(X_i, y_i, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Fit the vectorizer to the training data and transform it into vectors of word frequencies\n",
    "# vectorizer.fit(X_train_i)\n",
    "# freq_train_i = vectorizer.transform(X_train_i)\n",
    "# freq_test_i = vectorizer.transform(X_test_i)\n",
    "\n",
    "# # Train the LinearSVC classifier on the training data\n",
    "# svc = LinearSVC()\n",
    "# svc.fit(freq_train_i, y_train_i)\n",
    "\n",
    "# # Make predictions on the testing data\n",
    "# y_pred_i = svc.predict(freq_test_i)\n",
    "\n",
    "# # Calculate the accuracy of the classifier\n",
    "# accuracy = accuracy_score(y_test_i, y_pred_i)\n",
    "# print(\"Accuracy:\", accuracy)\n",
    "# print(classification_report(y_test_i, y_pred_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import ClassPredictionError\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(data, label):\n",
    "            \n",
    "    data = data['T1'] + data['T2']\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, label, stratify = label, test_size = 0.2, random_state = 1234)\n",
    "    \n",
    "    clf = RandomForestClassifier(random_state=1234,n_estimators=150,n_jobs=-1)    \n",
    "    v = TfidfVectorizer()\n",
    "    \n",
    "    train_corpus = X_train\n",
    "    test_corpus = X_test\n",
    "    \n",
    "    train_vector = v.fit_transform(train_corpus)\n",
    "    test_vector = v.transform(test_corpus)\n",
    "    \n",
    "    \n",
    "    fit = clf.fit(train_vector,y_train)\n",
    "    pred = clf.predict(test_vector)\n",
    "    \n",
    "    \n",
    "    Accuracy = accuracy_score(y_test,pred)\n",
    "    F1 = f1_score(y_test, pred, average='macro')\n",
    "    print(\"Accuracy:\", Accuracy)\n",
    "    print('F1:', F1)\n",
    "    \n",
    "    rec = recall_score(y_test, pred, average='macro')\n",
    "    print('Recall: ', rec)\n",
    "    prec = precision_score(y_test, pred, average='macro')\n",
    "    print('Precision: ', prec)\n",
    "    \n",
    "    \n",
    "    return y_test, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5260416666666666\n",
      "F1: 0.48821236374884436\n",
      "Recall:  0.5021441972661485\n",
      "Precision:  0.5026077291381669\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2249    1.0\n",
       " 903     1.0\n",
       " 4737    1.0\n",
       " 3749    0.0\n",
       " 4556    1.0\n",
       "        ... \n",
       " 2451    1.0\n",
       " 3231    0.0\n",
       " 3514    0.0\n",
       " 3339    0.0\n",
       " 2951    1.0\n",
       " Name: class, Length: 960, dtype: float64,\n",
       " array([1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "        0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
       "        0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
       "        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
       "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1.,\n",
       "        0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1.,\n",
       "        1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1.,\n",
       "        1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1.,\n",
       "        1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,\n",
       "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
       "        1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
       "        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0.,\n",
       "        1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
       "        0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
       "        1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
       "        1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0.,\n",
       "        1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0.,\n",
       "        1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
       "        0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
       "        1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
       "        0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "        0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0.,\n",
       "        1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1.,\n",
       "        0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
       "        0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "        1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1.,\n",
       "        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0.,\n",
       "        0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0.,\n",
       "        1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
       "        1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "        1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "        1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1.,\n",
       "        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
       "        0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "        1., 0., 1., 1., 1., 1., 1., 0.]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(balanced_p1[['T1', 'T2']], balanced_p1['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8835931207656228\n",
      "F1: 0.5134059958650313\n",
      "Recall:  0.5229436678735793\n",
      "Precision:  0.8735184687046242\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(47863    0.0\n",
       " 18812    1.0\n",
       " 22227    0.0\n",
       " 6241     0.0\n",
       " 71630    0.0\n",
       "         ... \n",
       " 60534    0.0\n",
       " 34950    0.0\n",
       " 76503    0.0\n",
       " 18232    1.0\n",
       " 28715    0.0\n",
       " Name: class, Length: 21107, dtype: float64,\n",
       " array([0., 0., 0., ..., 0., 0., 0.]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(binary_random[['T1', 'T2']], binary_random['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/arsh/anaconda3/lib/python3.9/site-packages\n"
     ]
    }
   ],
   "source": [
    "# ! pip install "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"[E002] Can't find factory for 'tok2vec'. This usually happens when spaCy calls `nlp.create_pipe` with a component name that's not built in - for example, when constructing the pipeline from a model's meta.json. If you're using a custom component, you can write to `Language.factories['tok2vec']` or remove it from the model meta and add it via `nlp.add_pipe` instead.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [56]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m data \u001b[38;5;241m=\u001b[39m balanced_p1\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# spacy_path = \"/home/arsh/anaconda3/lib/python3.9/site-packages\"\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# spacy.util.set_data_path(spacy_path)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Load spacy model\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men_core_web_sm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Define function to perform linguistic analysis\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlinguistic_analysis\u001b[39m(text):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/spacy/__init__.py:30\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m depr_path \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     29\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(Warnings\u001b[38;5;241m.\u001b[39mW001\u001b[38;5;241m.\u001b[39mformat(path\u001b[38;5;241m=\u001b[39mdepr_path), \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/spacy/util.py:168\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_lang_class(name\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblank:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))()\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m([d\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data_path\u001b[38;5;241m.\u001b[39miterdir()]):\n\u001b[0;32m--> 168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_model_from_link\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_package(name):  \u001b[38;5;66;03m# installed as package\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m load_model_from_package(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moverrides)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/spacy/util.py:185\u001b[0m, in \u001b[0;36mload_model_from_link\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE051\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n\u001b[0;32m--> 185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/en_core_web_sm/__init__.py:10\u001b[0m, in \u001b[0;36mload\u001b[0;34m(**overrides)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moverrides):\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_model_from_init_py\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;18;43m__file__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/spacy/util.py:239\u001b[0m, in \u001b[0;36mload_model_from_init_py\u001b[0;34m(init_file, **overrides)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE052\u001b[38;5;241m.\u001b[39mformat(path\u001b[38;5;241m=\u001b[39mpath2str(data_path)))\n\u001b[0;32m--> 239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_model_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/spacy/util.py:220\u001b[0m, in \u001b[0;36mload_model_from_path\u001b[0;34m(model_path, meta, **overrides)\u001b[0m\n\u001b[1;32m    218\u001b[0m         config\u001b[38;5;241m.\u001b[39mupdate(overrides)\n\u001b[1;32m    219\u001b[0m         factory \u001b[38;5;241m=\u001b[39m factories\u001b[38;5;241m.\u001b[39mget(name, name)\n\u001b[0;32m--> 220\u001b[0m         component \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_pipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfactory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m         nlp\u001b[38;5;241m.\u001b[39madd_pipe(component, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nlp\u001b[38;5;241m.\u001b[39mfrom_disk(model_path, exclude\u001b[38;5;241m=\u001b[39mdisable)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/spacy/language.py:310\u001b[0m, in \u001b[0;36mLanguage.create_pipe\u001b[0;34m(self, name, config)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE108\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 310\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE002\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n\u001b[1;32m    311\u001b[0m factory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfactories[name]\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m factory(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"[E002] Can't find factory for 'tok2vec'. This usually happens when spaCy calls `nlp.create_pipe` with a component name that's not built in - for example, when constructing the pipeline from a model's meta.json. If you're using a custom component, you can write to `Language.factories['tok2vec']` or remove it from the model meta and add it via `nlp.add_pipe` instead.\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load data\n",
    "data = balanced_p1\n",
    "# spacy_path = \"/home/arsh/anaconda3/lib/python3.9/site-packages\"\n",
    "# spacy.util.set_data_path(spacy_path)\n",
    "\n",
    "# Load spacy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Define function to perform linguistic analysis\n",
    "def linguistic_analysis(text):\n",
    "    doc = nlp(text)\n",
    "    lemmas = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]\n",
    "    pos_tags = [token.pos_ for token in doc if not token.is_stop and token.is_alpha]\n",
    "    ents = [ent.label_ for ent in doc.ents]\n",
    "    return ' '.join(lemmas), ' '.join(pos_tags), ' '.join(ents)\n",
    "\n",
    "# Apply linguistic analysis to text data\n",
    "data['T1_lemmas'], data['T1_pos_tags'], data['T1_ents'] = zip(*data['T1'].apply(linguistic_analysis))\n",
    "data['T2_lemmas'], data['T2_pos_tags'], data['T2_ents'] = zip(*data['T2'].apply(linguistic_analysis))\n",
    "\n",
    "# Vectorize text data using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(data['T1_lemmas'] + ' ' + data['T2_lemmas'])\n",
    "y = data['class']\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train SVM model\n",
    "svm = SVC(kernel='linear')\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on validation set\n",
    "y_pred = svm.predict(X_val)\n",
    "\n",
    "# Evaluate model performance\n",
    "print('Accuracy:', accuracy_score(y_val, y_pred))\n",
    "print('Precision:', precision_score(y_val, y_pred))\n",
    "print('Recall:', recall_score(y_val, y_pred))\n",
    "print('F1 score:', f1_score(y_val, y_pred))\n",
    "\n",
    "# Compute confusion matrix\n",
    "confusion_matrix = pd.crosstab(y_val, y_pred, rownames=['Actual'], colnames=['Predicted'])\n",
    "print('Confusion matrix:\\n', confusion_matrix)\n",
    "\n",
    "# Compute class-wise precision, recall, and F1 score\n",
    "class_precision = precision_score(y_val, y_pred, average=None)\n",
    "class_recall = recall_score(y_val, y_pred, average=None)\n",
    "class_f1 = f1_score(y_val, y_pred, average=None)\n",
    "print('Class-wise precision:\\n', class_precision)\n",
    "print('Class-wise recall:\\n', class_recall)\n",
    "print('Class-wise F1 score:\\n', class_f1)\n",
    "\n",
    "# Compute macro and micro precision, recall, and F1 score\n",
    "macro_precision = precision_score(y_val, y_pred, average='macro')\n",
    "macro_recall = recall_score(y_val, y_pred, average='macro')\n",
    "macro_f1 = f1_score(y_val, y_pred, average='macro')\n",
    "micro_precision = precision_score(y_val, y_pred, average='micro')\n",
    "micro_recall = recall_score(y_val, y_pred, average='micro')\n",
    "micro_f1 = f1_score(y_val, y_pred, average='micro')\n",
    "print('Macro precision:', macro_precision)\n",
    "print('Macro recall:', macro_recall)\n",
    "print('Macro F1 score:', macro_f1)\n",
    "print('Micro precision:', micro_precision)\n",
    "print('Micro recall:', micro_recall)\n",
    "print('Micro F1 score:', micro_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def get_linguistic_features(text):\n",
    "    # Tokenize the text into sentences and words\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    words = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "    # Get the number of sentences and words\n",
    "    num_sentences = len(sentences)\n",
    "    num_words = len(words)\n",
    "\n",
    "    # Get the average length of sentences and words\n",
    "    avg_sentence_length = num_words / num_sentences if num_sentences > 0 else 0\n",
    "    avg_word_length = sum(len(word) for word in words) / num_words if num_words > 0 else 0\n",
    "\n",
    "    # Get the number of unique words\n",
    "    unique_words = set([word.lower() for sent in words for word in sent])\n",
    "    num_unique_words = len(unique_words)\n",
    "\n",
    "    # Get the lexical diversity (i.e., ratio of unique words to total words)\n",
    "    lexical_diversity = num_unique_words / num_words if num_words > 0 else 0\n",
    "\n",
    "    # Get the number of noun phrases\n",
    "    noun_phrases = []\n",
    "    for sent in sentences:\n",
    "        pos_tags = nltk.pos_tag(nltk.word_tokenize(sent))\n",
    "        chunk_parser = nltk.RegexpParser('NP: {<DT>?<JJ>*<NN>}')\n",
    "        chunks = chunk_parser.parse(pos_tags)\n",
    "        for subtree in chunks.subtrees():\n",
    "            if subtree.label() == 'NP':\n",
    "                noun_phrases.append(' '.join(word for word, tag in subtree.leaves()))\n",
    "    num_noun_phrases = len(noun_phrases)\n",
    "\n",
    "    return [num_sentences, num_words, avg_sentence_length, avg_word_length, num_unique_words, lexical_diversity, num_noun_phrases]\n",
    "\n",
    "def classify(data, label):\n",
    "    # Concatenate T1 and T2 into a single text\n",
    "    data_text = data['T1'] + ' ' + data['T2']\n",
    "\n",
    "    # Extract linguistic features\n",
    "    features = np.array([get_linguistic_features(text) for text in data_text])\n",
    "\n",
    "    # Split data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, label, stratify=label, test_size=0.2, random_state=1234)\n",
    "\n",
    "    # Create a TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Convert the text data into vectors\n",
    "    train_vectors = vectorizer.fit_transform(data_text.iloc[X_train])\n",
    "    test_vectors = vectorizer.transform(data_text.iloc[X_test])\n",
    "\n",
    "\n",
    "\n",
    "    # Train an SVM classifier\n",
    "    clf = SVC(kernel='linear', random_state=1234)\n",
    "    clf.fit(train_vectors, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    pred = clf.predict(test_vectors)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, pred)\n",
    "    f1 = f1_score(y_test, pred, average='macro')\n",
    "    recall = recall_score(y_test, pred, average='macro')\n",
    "    precision = precision_score(y_test, pred, average='macro')\n",
    "\n",
    "    # Print evaluation metrics\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"Precision:\", precision)\n",
    "\n",
    "    return y_test, pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify(balanced_p1[['T1', 'T2']], balanced_p1['class'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [60]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mclassify\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbinary_random\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mT1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mT2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary_random\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclass\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [58]\u001b[0m, in \u001b[0;36mclassify\u001b[0;34m(data, label)\u001b[0m\n\u001b[1;32m     45\u001b[0m data_text \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mT1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mT2\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Extract linguistic features\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([get_linguistic_features(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m data_text])\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Split data into training and test sets\u001b[39;00m\n\u001b[1;32m     51\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(features, label, stratify\u001b[38;5;241m=\u001b[39mlabel, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1234\u001b[39m)\n",
      "Input \u001b[0;32mIn [58]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     45\u001b[0m data_text \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mT1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mT2\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Extract linguistic features\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[43mget_linguistic_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m data_text])\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Split data into training and test sets\u001b[39;00m\n\u001b[1;32m     51\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(features, label, stratify\u001b[38;5;241m=\u001b[39mlabel, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1234\u001b[39m)\n",
      "Input \u001b[0;32mIn [58]\u001b[0m, in \u001b[0;36mget_linguistic_features\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     31\u001b[0m noun_phrases \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences:\n\u001b[0;32m---> 33\u001b[0m     pos_tags \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     chunk_parser \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mRegexpParser(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNP: \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m<DT>?<JJ>*<NN>}\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     35\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m chunk_parser\u001b[38;5;241m.\u001b[39mparse(pos_tags)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/nltk/tag/__init__.py:166\u001b[0m, in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03mUse NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;03mtag the given list of tokens.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m:rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    165\u001b[0m tagger \u001b[38;5;241m=\u001b[39m _get_tagger(lang)\n\u001b[0;32m--> 166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_pos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtagset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtagger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/nltk/tag/__init__.py:123\u001b[0m, in \u001b[0;36m_pos_tag\u001b[0;34m(tokens, tagset, tagger, lang)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens: expected a list of strings, got a string\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 123\u001b[0m     tagged_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtagger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tagset:  \u001b[38;5;66;03m# Maps to the specified tagset.\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m lang \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/nltk/tag/perceptron.py:187\u001b[0m, in \u001b[0;36mPerceptronTagger.tag\u001b[0;34m(self, tokens, return_conf, use_tagdict)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tag:\n\u001b[1;32m    186\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_features(i, word, context, prev, prev2)\n\u001b[0;32m--> 187\u001b[0m     tag, conf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_conf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m output\u001b[38;5;241m.\u001b[39mappend((word, tag, conf) \u001b[38;5;28;01mif\u001b[39;00m return_conf \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m (word, tag))\n\u001b[1;32m    190\u001b[0m prev2 \u001b[38;5;241m=\u001b[39m prev\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/nltk/tag/perceptron.py:66\u001b[0m, in \u001b[0;36mAveragedPerceptron.predict\u001b[0;34m(self, features, return_conf)\u001b[0m\n\u001b[1;32m     64\u001b[0m     weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights[feat]\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m label, weight \u001b[38;5;129;01min\u001b[39;00m weights\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 66\u001b[0m         scores[label] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m value \u001b[38;5;241m*\u001b[39m weight\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Do a secondary alphabetic sort, for stability\u001b[39;00m\n\u001b[1;32m     69\u001b[0m best_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m label: (scores[label], label))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "classify(binary_random[['T1', 'T2']], binary_random['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Load the balanced_p1 dataset\n",
    "data = pd.read_csv('balanced_p1.csv')\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load the English language model in spaCy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Define a function to extract linguistic features from text\n",
    "def extract_features(text):\n",
    "    doc = nlp(text)\n",
    "    # Count the number of proper nouns in the text\n",
    "    num_proper_nouns = sum([1 for token in doc if token.pos_ == 'PROPN'])\n",
    "    # Count the number of nouns in the text\n",
    "    num_nouns = sum([1 for token in doc if token.pos_ == 'NOUN'])\n",
    "    # Count the number of verbs in the text\n",
    "    num_verbs = sum([1 for token in doc if token.pos_ == 'VERB'])\n",
    "    # Count the number of adjectives in the text\n",
    "    num_adjectives = sum([1 for token in doc if token.pos_ == 'ADJ'])\n",
    "    # Return a dictionary of the linguistic features\n",
    "    return {'num_proper_nouns': num_proper_nouns,\n",
    "            'num_nouns': num_nouns,\n",
    "            'num_verbs': num_verbs,\n",
    "            'num_adjectives': num_adjectives}\n",
    "\n",
    "# Vectorize the text data using TfidfVectorizer and the extract_features function\n",
    "vectorizer = TfidfVectorizer(analyzer='word', tokenizer=lambda x: x.split(), preprocessor=extract_features)\n",
    "X_train = vectorizer.fit_transform(train['T1'] + train['T2'])\n",
    "X_test = vectorizer.transform(test['T1'] + test['T2'])\n",
    "\n",
    "# Create the target variable\n",
    "y_train = train['class']\n",
    "y_test = test['class']\n",
    "\n",
    "# Train a Support Vector Machine (SVM) classifier on the training set\n",
    "svm = SVC(kernel='linear')\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set and evaluate the accuracy\n",
    "y_pred = svm.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
